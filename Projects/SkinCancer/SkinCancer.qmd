---
title: "Predicting Skin Cancer Using Statistical Learning Models"
subtitle: "A Comparative Analysis of Logistic, LASSO, and Elastic Net Regression on Structured Clinical Data"
description: >
  This project applies statistical learning methods to classify skin lesions as
  benign or malignant using structured, non-image clinical and demographic data.
  Logistic regression, LASSO, and elastic net models are compared, with model
  selection based on cross-validation and predictive performance.
author:
  - Franklin Truong
  - Jason Li
  - Roy Cheng
  - Keivan Bolouri
  - Felipe Duenas
date: 2025-12-15
categories:
  - Statistical Learning
  - Medical Data Analysis
  - Skin Cancer Prediction
image: "SkinCancer.jpg"
format:
  html:
    toc: true
bibliography: references.bib
csl: elsevier-vancouver.csl
cite-method: citeproc
---

![](Skincancer.jpg){width="4cm" height="4cm"}

[⬇ Download PDF](./FinalReport.pdf){.btn .btn-primary}

# Abstract

::: text-right
This project develops and evaluates statistical learning models to classify skin lesions as benign or malignant using structured (non-image) predictors. Using a skin cancer dataset with 50,000 training observations and 20,000 test observations, we performed exploratory analysis, handled missing values via median (numeric) and mode (categorical) imputation, standardized numerical features, and reduced dimensionality through association-based feature screening. We then trained and compared a baseline logistic regression model with regularized variants (LASSO and elastic net logistic regression), tuning hyperparameters via cross-validation to balance generalization and performance under slight class imbalance.

The final model was an elastic net logistic regression with mixing parameter $\alpha = 0.7$ and 34 predictors, selected based on leaderboard performance. On the held-out test set, the model achieved 60.508% accuracy, outperforming naive baselines but remaining modest for a diagnostic setting. Overall, results suggest that structured demographic, behavioral, environmental, and clinical variables provide limited discriminative signal without lesion imaging features, highlighting both the potential utility of tabular predictors and the limitations of relying on them alone for skin cancer detection.
:::

# Introduction

Skin cancer is one of the most common and preventable forms of cancer in the United States [@hhs_skin_cancer] , yet early detection remains crucial for improving survival outcomes. In the U.S. alone, nearly six million individuals are treated for skin cancer annually[@cdc_skin_cancer] , with melanoma accounting for approximately 97,000 new cases and over 8,000 deaths each year [@cdc_skin_cancer]. Early detection greatly improves patient prognosis – for example, the five-year survival rate for melanoma is over 99% when detected at an early stage, compared to much lower survival if the cancer has advanced[@skincancer_facts]. While skin cancer can often be identified visually or through clinical examination, early diagnosis remains challenging due to the wide range of risk factors and lesion characteristics that must be considered.

In this project, we analyze a skin cancer dataset containing 50,000 training observations and 20,000 test observations, with a binary response variable (Cancer) indicating whether a lesion is benign or malignant. The dataset includes 50 predictor variables representing a broad range of factors, including demographic information, environmental measures, behavioral and lifestyle factors, and clinical attributes. These predictors provide a structured, non-image-based view of potential skin cancer risk factors.

The goal of this project is to determine whether structured (tabular) data can be used to distinguish between benign and malignant skin lesions. By applying statistical learning methods, we aim to identify which predictors are most informative and to evaluate how well different classification models perform on this task. We seek to select an approach that balances interpretability, generalization, and predictive accuracy, while also discussing the limitations of using only structured predictors for skin cancer classification

# Literature Review

Recent studies have demonstrated the potential of machine learning models applied to structured clinical and demographic data (non-image features) for skin cancer risk prediction. For example, an XGBoost-based model using electronic health records and genetic/lifestyle factors from a 400,000-patient cohort achieved high accuracy in identifying skin cancer cases ($F_1 \approx 0.90$ in European-ancestry patients) and leveraged SHAP values to interpret nonlinear risk factor effects [@kaiser2020] . Another approach employed logistic regression to develop a nomogram with eight behavioral and dietary risk factors, showing good discrimination ($\text{AUC} \approx 0.8$) and clinical utility for head and neck skin cancer prevention[@Zou2025]. In melanoma-specific research, a new 16-factor risk model (MP16) was trained on a 41,000-person cohort and improved predictive accuracy ($\text{C-index} \approx 0.74$) compared to earlier tools, capturing $\sim 74\%$ of of future melanomas by targeting the top 40% high-risk group [@Whiteman2025]. Finally, a large 2021 study combined survey-derived risk variables with polygenic risk scores to create composite risk metrics for melanoma and non-melanoma skin cancers, identifying top-percentile individuals with over tenfold higher risk to inform targeted screening [@Fontanillas2021].

# Data Analysis

## Exploratory Data Analysis

We began by examining the overall structure and distributions in the training dataset. The response variable (Cancer) is binary (Benign or Malignant), and we observed a slight class imbalance: benign lesions were more frequent than malignant lesions in the training data. This imbalance means that a naive classifier that predicts all cases as benign would achieve a non-trivial accuracy, so careful model evaluation is necessary. We also inspected the distribution of key predictors. For example, the average age of patients with malignant lesions appeared higher than that of patients with benign lesions, consistent with the fact that skin cancer risk increases with age. We explored categorical risk factors as well: certain exposure-related factors (such as indicators of high UV exposure or tanning habits) were somewhat more prevalent in malignant cases, though there was significant overlap between the benign and malignant groups for most individual predictors. Overall, no single predictor showed a dramatic separation between malignant and benign lesions in isolation, suggesting that multiple factors in combination would be needed for effective prediction. In addition, we checked for missing data and outliers during the exploratory phase. We found that the dataset’s overall quality was high, with only a moderate amount of missing values (on the order of 7–8% missing per predictor). There was no evidence of extreme outliers that would require removal or transformation beyond standardization. The presence of some missing values and the lack of obvious one-variable predictors of cancer underlined the need for a robust modeling approach with proper data preprocessing, which we implemented as described below.  


```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
library(readr)
library(dplyr)
library(knitr)

# Read training dataset
train_clean <- read_csv("SkinCancerTrain_Clean.csv", show_col_types = FALSE)

# Ensure Cancer is a factor
train_clean <- train_clean %>%
  mutate(Cancer = factor(Cancer, levels = c("Benign", "Malignant")))

# Create count table
cancer_table <- train_clean %>%
  count(Cancer) %>%
  rename(
    `Cancer Type` = Cancer,
    `Number of Samples` = n
  )

# Display as formatted table
kable(cancer_table, align = "lc")

```


```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false

library(readr)
library(dplyr)
library(ggplot2)

# -------------------------
# Read training dataset
# -------------------------
train_clean <- read_csv(
  "SkinCancerTrain_Clean.csv",
  show_col_types = FALSE
)

# -------------------------
# Ensure Cancer is a factor
# -------------------------
train_clean <- train_clean %>%
  mutate(
    Cancer = factor(Cancer, levels = c("Benign", "Malignant"))
  )

# -------------------------
# Create count table (optional)
# -------------------------
cancer_table <- train_clean %>%
  count(Cancer)

# -------------------------
# Plot Benign vs Malignant
# -------------------------
ggplot(train_clean, aes(x = Cancer, fill = Cancer)) +
  geom_bar(width = 0.6) +
  scale_fill_manual(
    values = c(
      "Benign" = "#F8766D",
      "Malignant" = "#00BFC4"
    )
  ) +
  labs(
    x = "",
    y = "Count"
  ) +
  theme_classic(base_size = 14) +
  theme(
    legend.position = "none"
  )

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
#| fig-width: 15
#| fig-height: 18

library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(scales)

# -------------------------
# Read CLEAN training data
# -------------------------
train_clean <- read_csv("SkinCancerTrain_Clean.csv")

# -------------------------
# Variables to plot (categorical removed)
# -------------------------
vars_to_plot <- c(
  "age",
  "avg_daily_uv",
  "number_of_lesions",
  "sunburns_last_year",
  "lesion_size_mm",
  "sunscreen_spf",
  "years_lived_at_address",
  "income",
  "desk_height_cm",
  "residence_lon",
  "exercise_freq_per_week",
  "zip_code_last_digit",
  "distance_from_beach_km",
  "alcohol_drinks_per_week",
  "BMI",
  "commute_minutes",
  "frequency_doctor_visits_per_year",
  "residence_lat",
  "monthly_screen_time_minutes"
)

# -------------------------
# Long format + log only for income
# -------------------------
train_long <- train_clean %>%
  select(all_of(vars_to_plot), Cancer) %>%
  pivot_longer(
    cols = all_of(vars_to_plot),
    names_to = "variable",
    values_to = "value"
  ) %>%
  mutate(
    value_plot = if_else(
      variable == "income",
      log10(value + 1),
      value
    )
  )

# -------------------------
# Wilcoxon p-values per variable
# -------------------------
pvals <- train_long %>%
  group_by(variable) %>%
  summarise(
    p_value = wilcox.test(value ~ Cancer)$p.value,
    .groups = "drop"
  ) %>%
  mutate(
    p_label = case_when(
      p_value < 0.001 ~ "p < 0.001",
      TRUE ~ paste0("p = ", signif(p_value, 3))
    )
  )

# -------------------------
# Merge p-values back
# -------------------------
train_long <- train_long %>%
  left_join(pvals, by = "variable")

# -------------------------
# Final plot (3 columns → no empty space)
# -------------------------
ggplot(train_long, aes(x = Cancer, y = value_plot, fill = Cancer)) +
  geom_boxplot(
    alpha = 0.75,
    width = 0.6,
    outlier.alpha = 0
  ) +
  facet_wrap(~variable, ncol = 3, scales = "free_y") +
  geom_text(
    data = pvals,
    aes(x = 1.5, y = Inf, label = p_label),
    inherit.aes = FALSE,
    vjust = 1.4,
    size = 3.5
  ) +
  labs(
    title = "Clean Training Data: Malignant vs Benign (Wilcoxon Test)",
    x = "Cancer Type",
    y = "Value (log scale for Income)"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "none",
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.x = element_text(size = 11),
    axis.text.y = element_text(size = 10),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  scale_y_continuous(
    labels = label_number(scale_cut = cut_si(""))
  )

```






```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(scales)

# -------------------------
# Read data
# -------------------------
train_clean <- read_csv("SkinCancerTrain_Clean.csv", show_col_types = FALSE)

# -------------------------
# Ensure correct types
# -------------------------
train_clean <- train_clean %>%
  mutate(
    Cancer = factor(Cancer, levels = c("Benign", "Malignant")),
    participates_outdoor_sports = factor(participates_outdoor_sports,
                                         levels = c("No", "Yes")),
    vitamin_d_supplement = factor(vitamin_d_supplement,
                                  levels = c("No", "Yes")),
    immunosuppressed = factor(immunosuppressed,
                              levels = c("No", "Yes")),
    outdoor_job = factor(outdoor_job,
                         levels = c(0, 1),
                         labels = c("No", "Yes"))
  )

# -------------------------
# Prepare data
# -------------------------
plot_data <- train_clean %>%
  select(
    participates_outdoor_sports,
    vitamin_d_supplement,
    immunosuppressed,
    outdoor_job,
    Cancer
  ) %>%
  pivot_longer(
    cols = -Cancer,
    names_to = "variable",
    values_to = "value"
  )

# -------------------------
# BEAUTIFUL plot (percent stacked)
# -------------------------
ggplot(plot_data, aes(x = value, fill = Cancer)) +
  geom_bar(position = "fill", width = 0.7) +
  facet_wrap(
    ~variable,
    nrow = 1,
    labeller = as_labeller(c(
      participates_outdoor_sports = "Outdoor Sports",
      vitamin_d_supplement       = "Vitamin D Sup",
      immunosuppressed           = "Immuno-suppr",
      outdoor_job                = "Outdoor Job"
    ))
  ) +
  scale_y_continuous(labels = percent_format()) +
  scale_fill_manual(
    values = c(
      "Benign" = "#F8766D",
      "Malignant" = "#00BFC4"
    )
  ) +
  labs(
    x = NULL,
    y = "Proportion"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    strip.text = element_text(face = "bold"),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  )

```














## Data Cleaning and Missing Values

After completing the exploratory analysis, we examined the dataset for missing values. We found that the overall data quality was relatively high, with most predictors containing approximately 7–8% missing values. No predictor exceeded 10% missingness, so eliminating variables or observations would have resulted in unnecessary data loss. To address missing values in a consistent manner, we applied the following imputation strategy: Numerical variables were imputed using the median, which is robust to outliers. Categorical variables were imputed using the most frequent category (mode). This approach allowed us to preserve all 50,000 observations in the training dataset while ensuring that the data were suitable for modeling. After imputation, no missing values remained in the predictors used for analysis.

```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false

library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(cowplot)

# -------------------------
# Read data
# -------------------------
train_not_clean <- read_csv("SkinCancerTrain.csv")
train_clean     <- read_csv("SkinCancerTrain_Clean.csv")

test_not_clean  <- read_csv("SkinCancerTestNoY.csv")
test_clean      <- read_csv("SkinCancerTestNoY_Clean.csv")

# -------------------------
# Variables to plot
# -------------------------
vars_to_plot <- c(
  "age", "avg_daily_uv", "number_of_lesions", "sunburns_last_year",
  "outdoor_job", "lesion_size_mm", "sunscreen_spf",
  "years_lived_at_address", "income", "desk_height_cm",
  "residence_lon", "exercise_freq_per_week", "zip_code_last_digit",
  "distance_from_beach_km", "alcohol_drinks_per_week", "BMI",
  "commute_minutes", "frequency_doctor_visits_per_year",
  "residence_lat", "monthly_screen_time_minutes"
)

# -------------------------
# Human-readable labels
# -------------------------
var_labels <- c(
  age = "Age (years)",
  avg_daily_uv = "Average Daily UV Exposure",
  number_of_lesions = "Number of Skin Lesions",
  sunburns_last_year = "Sunburns Last Year",
  outdoor_job = "Outdoor Job",
  lesion_size_mm = "Lesion Size (mm)",
  sunscreen_spf = "Sunscreen SPF",
  years_lived_at_address = "Years Lived at Address",
  income = "Annual Income",
  desk_height_cm = "Desk Height (cm)",
  residence_lon = "Residence Longitude",
  exercise_freq_per_week = "Exercise Frequency (per week)",
  zip_code_last_digit = "ZIP Code (Last Digit)",
  distance_from_beach_km = "Distance from Beach (km)",
  alcohol_drinks_per_week = "Alcohol Drinks per Week",
  BMI = "Body Mass Index (BMI)",
  commute_minutes = "Commute Time (minutes)",
  frequency_doctor_visits_per_year = "Doctor Visits per Year",
  residence_lat = "Residence Latitude",
  monthly_screen_time_minutes = "Monthly Screen Time (minutes)"
)

# -------------------------
# Fixed colors (IMPORTANT)
# -------------------------
status_colors <- c(
  "Clean" = "#F8766D",
  "Not Clean" = "#00BFC4"
)

# -------------------------
# Keep only selected variables
# -------------------------
train_num_not_clean <- train_not_clean %>% select(all_of(vars_to_plot))
train_num_clean     <- train_clean     %>% select(all_of(vars_to_plot))

test_num_not_clean  <- test_not_clean  %>% select(all_of(vars_to_plot))
test_num_clean      <- test_clean      %>% select(all_of(vars_to_plot))

# -------------------------
# PRINT LEGEND ONCE (BEFORE PLOTS)
# -------------------------
legend_plot <- ggplot(
  tibble(x = c(1, 2), y = c(1, 1), status = c("Clean", "Not Clean")),
  aes(x = x, y = y, fill = status, color = status)
) +
  geom_tile(width = 0.4, height = 0.4) +
  scale_fill_manual(values = status_colors) +
  scale_color_manual(values = status_colors) +
  theme_void() +
  theme(
    legend.position = "top",
    legend.justification = "center",
    legend.title = element_blank()
  )

print(get_legend(legend_plot))

# -------------------------
# Function: plots WITHOUT legend
# -------------------------
compare_distribution_4up <- function(not_clean, clean, sample_n = 2000) {

  plots <- list()

  for (v in vars_to_plot) {

    x1 <- not_clean[[v]]
    x2 <- clean[[v]]

    if (length(x1) > sample_n) x1 <- sample(x1, sample_n)
    if (length(x2) > sample_n) x2 <- sample(x2, sample_n)

    df <- tibble(
      value  = c(x1, x2),
      status = rep(c("Not Clean", "Clean"),
                   c(length(x1), length(x2)))
    )

    p <- ggplot(df, aes(x = value, fill = status, color = status)) +
      geom_density(alpha = 0.3, adjust = 1.2, na.rm = TRUE) +
      scale_fill_manual(values = status_colors) +
      scale_color_manual(values = status_colors) +
      labs(
        x = var_labels[v],
        y = "Density"
      ) +
      theme_minimal(base_size = 12) +
      theme(
        legend.position = "none"
      )

    plots[[length(plots) + 1]] <- p
  }

  # Print 4 plots per page
  for (i in seq(1, length(plots), by = 4)) {
    print(
      plot_grid(
        plotlist = plots[i:min(i + 3, length(plots))],
        ncol = 2
      )
    )
  }
}

# -------------------------
# RUN
# -------------------------
set.seed(123)

compare_distribution_4up(train_num_not_clean, train_num_clean)
compare_distribution_4up(test_num_not_clean,  test_num_clean)


```




```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
#| results: "hide"
library(readr)
library(dplyr)
library(tidyr)
# Set file paths (adjust if needed)
train_path <- "SkinCancerTrain.csv"
test_path  <- "SkinCancerTestNoY.csv"

# Read CSV files
train_data <- read_csv(train_path)
test_data  <- read_csv(test_path)
missing_summary <- function(df) {
  df %>%
    summarise(across(
      everything(),
      ~ sum(is.na(.))
    )) %>%
    pivot_longer(
      cols = everything(),
      names_to = "Variable",
      values_to = "Missing_Count"
    ) %>%
    mutate(
      Missing_Percent = round(
        100 * Missing_Count / nrow(df), 2
      )
    ) %>%
    arrange(desc(Missing_Count))
}
# Missing values in training data
missing_train <- missing_summary(train_data)

# Missing values in test data
missing_test <- missing_summary(test_data)
missing_train
missing_test

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
#| results: "hide"
missing_report <- missing_train %>%
  rename(
    Missing_Count_Train   = Missing_Count,
    Missing_Percent_Train = Missing_Percent
  ) %>%
  left_join(
    missing_test %>%
      rename(
        Missing_Count_Test   = Missing_Count,
        Missing_Percent_Test = Missing_Percent
      ),
    by = "Variable"
  )
missing_report %>%
  summarise(
    Total_Variables = n(),
    Vars_Over_20pct_Train = sum(Missing_Percent_Train > 20, na.rm = TRUE),
    Vars_Over_30pct_Train = sum(Missing_Percent_Train > 30, na.rm = TRUE),
    Vars_Over_50pct_Train = sum(Missing_Percent_Train > 50, na.rm = TRUE),
    Vars_Over_20pct_Test  = sum(Missing_Percent_Test  > 20, na.rm = TRUE),
    Vars_Over_30pct_Test  = sum(Missing_Percent_Test  > 30, na.rm = TRUE),
    Vars_Over_50pct_Test  = sum(Missing_Percent_Test  > 50, na.rm = TRUE)
  )

```

::: {#tbl-missing-panel layout-ncol="2" tbl-cap="Top ten variables with the highest proportion of missing values."}
```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
library(knitr)
library(kableExtra)

missing_train %>%
  arrange(desc(Missing_Percent)) %>%
  slice(1:10) %>%
  select(Variable, Missing_Percent) %>%
  kable(
    col.names = c("Variable", "Missing (%)"),
    digits = 1,
    longtable = FALSE
  ) %>%
  kable_styling(full_width = FALSE) %>%
  footnote(
    general = "(a) Training set variables.",
    general_title = "",
    threeparttable = TRUE
  )

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
library(knitr)
library(kableExtra)
missing_test %>%
  arrange(desc(Missing_Percent)) %>%
  slice(1:10) %>%
  select(Variable, Missing_Percent) %>%
  kable(
    col.names = c("Variable", "Missing (%)"),
    digits = 1,
    longtable = FALSE
  ) %>%
  kable_styling(full_width = FALSE) %>%
  footnote(
    general = "(b) Test set variables.",
    general_title = "",
    threeparttable = TRUE
  )

```
:::

Using these association measures, all predictors were ranked from strongest to weakest. Variables with very weak association to the response variable were discarded, while stronger predictors were retained. This process reduced the dataset from 51 variables to 35 total variables, including the response variable. The final set consisted of 20 numerical predictors and 14 categorical predictors, while maintaining all original observations. This variable selection process helped simplify the modeling stage, reduced multicollinearity, and improved model interpretability, while retaining the most informative predictors for classification.

```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
#| fig-pos: "H"
#| fig-width: 8
#| fig-height: 8

library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)

# ---- Plot 1 (left) ----
p1 <- missing_train %>%
  arrange(Missing_Percent) %>%
  slice_tail(n = 15) %>%
  ggplot(aes(
    x = Missing_Percent,
    y = reorder(Variable, Missing_Percent)
  )) +
  geom_col() +
  labs(
    title = "(a) Training Set",
    x = "Missing Percentage",
    y = NULL
  ) +
  theme_minimal()

# ---- Plot 2 (right) ----
p2 <- missing_test %>%
  arrange(Missing_Percent) %>%
  slice_tail(n = 15) %>%
  ggplot(aes(
    x = Missing_Percent,
    y = reorder(Variable, Missing_Percent)
  )) +
  geom_col() +
  labs(
    title = "(b) Test Set",
    x = "Missing Percentage",
    y = NULL
  ) +
  theme_minimal()

# ---- Plot 3 (bottom, full width) ----
p3 <- missing_report %>%
  arrange(desc(Missing_Percent_Train)) %>%
  slice(1:15) %>%
  select(
    Variable,
    Missing_Percent_Train,
    Missing_Percent_Test
  ) %>%
  pivot_longer(
    cols = c(Missing_Percent_Train, Missing_Percent_Test),
    names_to = "Dataset",
    values_to = "Missing_Percent"
  ) %>%
  ggplot(aes(
    x = Missing_Percent,
    y = reorder(Variable, Missing_Percent),
    fill = Dataset
  )) +
  geom_col(position = "dodge") +
  guides(fill = guide_legend(nrow = 1)) +
  labs(
    title = "(c) Top 15 Variables by Missingness",
    x = "Missing Percentage",
    y = NULL
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.justification = "center"
  )

# ---- Layout: 2 on top, 1 below ----
(p1 | p2) / p3
```




```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
# Train data
train_not_clean <- read_csv("SkinCancerTrain.csv")
train_clean     <- read_csv("SkinCancerTrain_Clean.csv")

# Test data (NO target variable)
test_not_clean  <- read_csv("SkinCancerTestNoY.csv")
test_clean      <- read_csv("SkinCancerTestNoY_Clean.csv")
train_num_not_clean <- train_not_clean %>% select(where(is.numeric))
train_num_clean     <- train_clean %>% select(where(is.numeric))

test_num_not_clean  <- test_not_clean %>% select(where(is.numeric))
test_num_clean      <- test_clean %>% select(where(is.numeric))
compare_distribution <- function(not_clean, clean, dataset_name) {

  num_vars <- intersect(names(not_clean), names(clean))

  for (var in num_vars) {

    plot_data <- tibble(
      value = c(not_clean[[var]], clean[[var]]),
      status = rep(c("Not Clean", "Clean"),
                   c(length(not_clean[[var]]),
                     length(clean[[var]])))
    )

    p <- ggplot(plot_data, aes(x = value, color = status, fill = status)) +
      geom_density(alpha = 0.3, na.rm = TRUE) +
      labs(
        title = paste(dataset_name, "-", var),
        x = var,
        y = "Density"
      ) +
      theme_minimal()

    print(p)
  }
}
# Compare training data distributions
compare_distribution(
  not_clean = train_num_not_clean,
  clean     = train_num_clean,
  dataset_name = "TRAIN"
)


```














## Variable Selection

Given the large number of predictors, variable selection was necessary to reduce dimensionality and eliminate weak predictors that could introduce noise and increase the risk of overfitting. For categorical predictors, we used Cramér’s V to measure the strength of association between each categorical variable and the cancer outcome. Cramér’s V is based on the chi-squared statistic and produces values between 0 and 1, with larger values indicating stronger association. Here is a sample of the first 10 variables: For numerical predictors, we computed the point-biserial correlation, which measures the strength of association between a continuous predictor and a binary response variable. Numerical variables were ranked based on the absolute value of this correlation.

```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false

library(readr)
library(dplyr)
library(tidyr)
library(vcd)        # Cramér’s V
library(knitr)
library(kableExtra)

############################################################
# 1. Read clean datasets
############################################################

train_data <- read_csv("SkinCancerTrain_Clean.csv", show_col_types = FALSE)
test_data  <- read_csv("SkinCancerTestNoY_Clean.csv", show_col_types = FALSE)

############################################################
# 2. Specify outcome variable (TRAIN ONLY)
############################################################

outcome_var <- "Cancer"

if (!outcome_var %in% colnames(train_data)) {
  stop("Outcome variable not found in training data.")
}

############################################################
# 3. Recode outcome to binary (0/1)
############################################################

train_data <- train_data %>%
  mutate(
    Cancer = case_when(
      Cancer == "Malignant" ~ 1,
      Cancer == "Benign"    ~ 0,
      TRUE ~ NA_real_
    )
  )

if (any(is.na(train_data$Cancer))) {
  stop("NA values found in outcome variable after recoding.")
}

############################################################
# 4. Prepare predictors
############################################################

train_data <- train_data %>%
  mutate(across(where(is.character), as.factor))

test_data <- test_data %>%
  mutate(across(where(is.character), as.factor))

############################################################
# PART A: Categorical Variable Selection (Cramér’s V)
############################################################

cat_vars <- names(select(train_data, where(is.factor)))

compute_cramers_v <- function(var, outcome, data) {
  tbl <- table(data[[var]], data[[outcome]])
  suppressWarnings(assocstats(tbl)$cramer)
}

cramers_results <- tibble(variable = cat_vars) %>%
  mutate(
    cramersV = sapply(
      variable,
      compute_cramers_v,
      outcome = outcome_var,
      data = train_data
    )
  ) %>%
  arrange(desc(cramersV))

# ---- DISPLAY top 10 (reporting only) ----
cramers_results %>%
  slice_head(n = 10) %>%
  kable(
    col.names = c("Variable", "Cramér’s V"),
    digits = 3,
    caption = "Top 10 Categorical Predictors Ranked by Cramér’s V (Training Data)"
  ) %>%
  kable_styling(full_width = FALSE)

# ---- SELECT top 14 categorical predictors ----
selected_cat_vars <- cramers_results %>%
  slice_head(n = 14) %>%
  pull(variable)

############################################################
# PART B: Numerical Variable Selection (Point-Biserial)
############################################################

num_vars <- setdiff(
  names(select(train_data, where(is.numeric))),
  outcome_var
)

pb_results <- tibble(variable = num_vars) %>%
  mutate(
    point_biserial = sapply(
      variable,
      function(v) {
        cor(
          train_data[[v]],
          train_data[[outcome_var]],
          use = "complete.obs"
        )
      }
    ),
    abs_point_biserial = abs(point_biserial)
  ) %>%
  arrange(desc(abs_point_biserial))

# ---- DISPLAY top 10 (reporting only) ----
pb_results %>%
  slice_head(n = 10) %>%
  select(
    Variable = variable,
    `Point-Biserial Correlation` = point_biserial
  ) %>%
  kable(
    digits = 4,
    caption = "Top 10 Numerical Predictors Ranked by Point-Biserial Correlation (Training Data)"
  ) %>%
  kable_styling(full_width = FALSE)

# ---- SELECT top 20 numerical predictors ----
selected_num_vars <- pb_results %>%
  slice_head(n = 20) %>%
  pull(variable)

############################################################
# PART C: Final Variable Set
############################################################

selected_predictors <- c(selected_cat_vars, selected_num_vars)

# Final count check (should be 35 including outcome)
length(c(outcome_var, selected_predictors))





```


\newpage

```{r}
#| echo: false
#| message: false
#| warning: false
#| error: false

library(dplyr)
library(tibble)
library(knitr)
library(kableExtra)

# ---- Build compact final variable table with numbering ----

final_vars_table <- bind_rows(
  # Categorical variables (Top 14)
  cramers_results %>%
    slice_head(n = 14) %>%
    mutate(
      No = row_number(),
      Type = "Categorical",
      `Selection Criterion` = "Cramér’s V > 0.05"
    ) %>%
    select(No, Type, Variable = variable, `Selection Criterion`),

  # Numerical variables (Top 20)
  pb_results %>%
    slice_head(n = 20) %>%
    mutate(
      No = row_number(),
      Type = "Numerical",
      `Selection Criterion` = "Abs. Point-Biserial Corr. > 0.02"
    ) %>%
    select(No, Type, Variable = variable, `Selection Criterion`)
)

# ---- Render table ----
final_vars_table %>%
  kable(
    caption = "Final Selected Predictors After Association-Based Variable Screening",
    align = "ccll"
  ) %>%
  kable_styling(
    full_width = FALSE,
    position = "center",
    latex_options = c("hold_position", "striped")
  )


```

# Model Selection

After preprocessing and feature selection, we proceeded to build predictive classification models on the reduced set of 34 predictors. We first fit a standard logistic regression model using all selected features as a baseline. The logistic regression model provides an interpretable baseline by estimating the odds of a lesion being malignant as a linear function of the predictors (with a logistic link). However, with 34 predictors, a basic logistic model can still risk overfitting and may include some predictors that contribute little to accuracy. All numeric features were standardized (centered to mean 0 and scaled to unit variance) prior to modeling so that they would be on comparable scales and to help the optimization of regularized models. Next, we applied regularization techniques to the logistic model to perform automatic variable selection and to improve generalization performance. In particular, we trained a LASSO logistic regression model, which is a logistic regression with an L1 penalty on the coefficients. The LASSO penalty tends to shrink the coefficients of less important features toward zero, effectively eliminating those features from the model if they are not strongly associated with the outcome. This can greatly simplify the model by selecting a sparse set of predictors. We expected the LASSO to be useful given the number of predictors and the likelihood that some of them, even after our filtering step, might still be only weakly related to the outcome. Finally, we trained an elastic net regularized logistic regression model, which generalizes the LASSO by using a combination of L1 and L2 penalties. The elastic net has two hyperparameters to tune: the mixing parameter α (which controls the relative weight of L1 vs L2 regularization) and the regularization strength parameter λ. We utilized cross-validation to tune these hyperparameters and to select the best model. Specifically, we performed 25-fold cross-validation on the training set with a model training pipeline that included the standardization step and the logistic model with a given regularization setting. During cross-validation, we evaluated model performance using a classification metric that takes into account both sensitivity and specificity. (In practice, we optimized for the ROC AUC metric on the validation folds, which balances true positive rate and false positive rate, rather than simply using raw accuracy, to account for the slight class imbalance.) We searched over a grid of λ values for the LASSO and elastic net models, and for the elastic net we also tried different values of α to find the optimal blend of L1 and L2 regularization. Based on the cross-validation results, the elastic net model achieved the best performance on the validation folds, outperforming both the baseline logistic regression and the purely L1-penalized model. The optimal hyperparameters found for the elastic net were a mixing parameter of approximately $\alpha \approx 0.7$ (indicating that a 70% L1 and 30% L2 penalty mix gave the best result) and a certain regularization strength λ that maximized the model’s discriminative ability. The fact that $\alpha \approx 0.7$ was selected (rather than $\alpha = 1$ corresponding to pure LASSO) suggested that including some L2 penalty provided a benefit, likely by stabilizing the coefficient estimates for correlated predictors. After determining the best parameters, we refit the elastic net logistic regression on the entire training set using those parameters. This final model was then used to predict probabilities of malignancy for the lesions in the hold-out test set. We used a default 0.5 probability cutoff to classify a lesion as “Malignant” or “Benign” for the final submission. (We also briefly experimented with adjusting the classification threshold to maximize accuracy on the training data, but ultimately the standard 0.5 threshold was chosen for evaluating test outcomes.) The final submitted model, therefore, was an elastic net logistic regression with $\alpha \approx 0.7$, applied to 34 selected features, predicting the probability of a lesion being malignant.

# Results and Discussions

The final elastic net logistic regression model achieved a public accuracy of 0.606, which is substantially better than random guessing (approximately 0.50) and indicates meaningful predictive signal from the structured variables. However, an accuracy in the low 60% range remains modest for a medical diagnostic task, suggesting that non-image features alone provide limited discriminative power for skin cancer classification. This result highlights the inherent difficulty of distinguishing malignant from benign lesions using only demographic, environmental, and lifestyle variables, and suggests that incorporating lesion imaging or more informative clinical features would be necessary to achieve stronger predictive performance.

# Conclusion and Limitations

Overall, despite our model ranking in the top 15 on the project leaderboard, this approach faced several important limitations. First, the model relied exclusively on structured, non-imaging variables and did not make use of any image-based features of the lesions. In real-world skin cancer detection, the appearance of the lesion (shape, color, border irregularity, etc.) is critical, and excluding that information severely limits diagnostic accuracy. Second, there was a class imbalance (fewer malignant cases than benign), which may have negatively impacted the model’s ability to learn and detect the malignant class. Imbalanced data can lead models to favor the majority class (benign) and struggle to identify the minority class (malignant), as we observed with the lower sensitivity for malignancies. Finally, even after feature selection, we still included a couple dozen predictors that were derived from lifestyle and environmental data; many of these variables have only weak relationships with the outcome. The inclusion of numerous weak predictors can introduce noise and reduce the precision of the model’s predictions, even with regularization to mitigate overfitting. This project demonstrates that supervised machine learning methods can be applied to distinguish between benign and malignant skin lesions using structured patient and lesion data. However, the moderate accuracy of the final model highlights the limitations of relying solely on non-image predictors for skin cancer classification. In future work, model performance could likely be improved by incorporating image data (such as dermoscopy or clinical photographs of the lesions) alongside the structured variables, since computer vision algorithms and dermatologists alike rely heavily on visual patterns to identify skin cancers. Additionally, exploring interaction terms or non-linear models (e.g. decision tree ensembles or neural networks) might capture more complex relationships between risk factors. Careful handling of class imbalance (through techniques such as resampling, cost-sensitive learning, or adjusting decision thresholds) would also be important to improve malignant case detection. Nevertheless, this project provided valuable experience in the end-to-end process of predictive modeling with real-world data. We performed data cleaning and imputation to handle missing values, used statistical methods for feature selection, applied and tuned regularized logistic regression models, and evaluated the results on an independent test set. The exercise underscored the importance of domain knowledge (to understand what predictors might matter), the challenges of working with imperfect data, and the need to consider model limitations when interpreting results. Although our structured-data model alone is not ready for clinical use, it serves as a baseline and learning tool. It emphasizes that while non-image data can contribute some signal for skin cancer risk, truly effective skin cancer prediction will likely require integrating all available information — including the rich visual data that was beyond the scope of this project. The insights gained here set the stage for more comprehensive approaches in the future, combining both data-driven modeling and clinical expertise to improve early detection of skin cancer.

# Author Contributions.

All authors contributed equally to the conception and design of the study, data preprocessing and analysis, model development, interpretation of results, and writing and revision of the manuscript. All authors read and approved the final version of the manuscript.

# Acknowledgments

# References

::: {#refs}
:::
