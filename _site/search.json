[
  {
    "objectID": "projects_files/Courses.html#coursework-completed-and-in-progress",
    "href": "projects_files/Courses.html#coursework-completed-and-in-progress",
    "title": "Relevant Academic Coursework",
    "section": "Coursework Completed and In-Progress",
    "text": "Coursework Completed and In-Progress\n\nUpper Division and Graduate-Level Mathematics & Statistics Courses\n\n\n\n\n\n\n\n\n\n\nNo.\nCourse (Number & Title)\nTerm\nTextbook Ref\n\n\n\n\n1\nMATH 115A – Linear Algebra\nFall 2024\n[5]\n\n\n2\nSTATS 100A – Introduction to Probability\nFall 2024\n[25], [26]\n\n\n3\nMATH 151A – Applied Numerical Methods\nWinter 2025\n[7]\n\n\n4\nSTATS 100B – Mathematical Statistics\nWinter 2025\n[8]\n\n\n5\nSTATS 101A – Data Analysis & Regression\nWinter 2025\n[9], [10]\n\n\n6\nSTATS 100C – Linear Models\nSpring 2025\n[11], [31]\n\n\n7\nSTATS 101B – Design & Analysis of Experiments\nSpring 2025\n[12]\n\n\n8\nSTATS 102A – Intro to Computational Statistics (R)\nSummer 2025\n[21], [22], [23], [24]\n\n\n9\nMATH 164 – Optimization\nFall 2025\n[13]\n\n\n10\nSTATS 101C – Statistical Models & Data Mining\nFall 2025\n[14]\n\n\n11\nSTATS 102C – Monte Carlo Methods\nFall 2025\n[15]\n\n\n12\nMATH 156 – Machine Learning\nWinter 2026\n[16]\n\n\n13\nMATH 171 – Stochastic Processes\nWinter 2026\n[30]\n\n\n14\nSTATS 102B – Computation & Optimization for Statistics\nSpring 2026\n[18]\n\n\n15\nMATH 182 – Algorithms\nSpring 2026\n[29]"
  },
  {
    "objectID": "projects_files/Courses.html#lower-division-mathematics-statistics-courses",
    "href": "projects_files/Courses.html#lower-division-mathematics-statistics-courses",
    "title": "Relevant Academic Coursework",
    "section": "Lower Division Mathematics & Statistics Courses",
    "text": "Lower Division Mathematics & Statistics Courses\n\n\n\n\n\n\n\n\n\n\nNo.\nCourse (Number & Title)\nTerm\nTextbook Ref\n\n\n\n\n1\nMATH-227 – Statistics\nWinter 2022\n[27]\n\n\n2\nMATH-229 – Statistics for Data Science\nSummer 2024\n[28]\n\n\n3\nMATH 265 – Calculus I\nWinter 2022\n[1]\n\n\n4\nMATH 266 – Calculus II\nSpring 2022\n[1]\n\n\n5\nMATH 267 – Calculus III\nFall 2022\n[1]\n\n\n6\nMATH 270 – Linear Algebra\nFall 2022\n[2]\n\n\n7\nMATH 272 – Methods of Discrete Mathematics\nFall 2023\n[3]\n\n\n8\nMATH 275 – Ordinary Differential Equations\nSpring 2023\n[4]\n\n\n\n\n\n\n\n\n\n\nNoteShow Textbook References\n\n\n\n\n\nReferences\n[1] Stewart, James. Calculus: Early Transcendentals. 8th ed. Cengage, 2015.\n[2] Larson, Ron. Elementary Linear Algebra. 8th ed. Cengage, 2017.\n[3] Epp, Susanna S. Discrete Mathematics with Applications. 5th ed. Cengage, 2019.\n[4] Zill, Dennis G. A First Course in Differential Equations with Modeling Applications. 11th ed. Cengage, 2018.\n[5] Friedberg, Stephen H., Arnold J. Insel, and Lawrence E. Spence. Linear Algebra. 5th ed. Pearson, 2024.\n[6] Ross, Sheldon M. A First Course in Probability. 10th ed. Pearson, 2019.\n[7] Burden, Richard L., J. Douglas Faires, and Annette M. Burden. Numerical Analysis. 10th ed. Cengage, 2015.\n[8] Rice, John A. Mathematical Statistics and Data Analysis. 3rd ed. Cengage, 2006.\n[9] Sheather, Simon J. A Modern Approach to Regression with R. Springer, 2009.\n[10] Kutner, Michael H., et al. Applied Linear Statistical Models. 5th ed. McGraw-Hill, 2005.\n[11] Abraham, B., and J. Ledolter. Introduction to Regression Modeling. Duxbury, 2006.\n[12] Montgomery, Douglas C. Design and Analysis of Experiments. 9th ed. Wiley, 2017.\n[13] Boyd, Stephen, and Lieven Vandenberghe. Convex Optimization. Cambridge UP, 2004.\n[14] James, Gareth, et al. An Introduction to Statistical Learning with R. 2nd ed., Springer, 2021.\n[15] Robert, Christian P., and George Casella. Introducing Monte Carlo Methods with R. Springer, 2010.\n[16] Bishop, Christopher M. Pattern Recognition and Machine Learning. Springer, 2006.\n[17] Durrett, Rick. Essentials of Stochastic Processes. 2nd ed. Springer, 2012.\n[18] Zhou, Qing. Course Notes for STATS 102B. UCLA.\n[19] Isaaks, Edward H., and R. Mohan Srivastava. An Introduction to Applied Geostatistics. Oxford UP, 1989.\n[20] UCLA Statistics 100C course handouts.\nhttp://www.stat.ucla.edu/~nchristo/statistics100C/\n[21] Wickham, Hadley. Advanced R. 2nd ed. Chapman & Hall/CRC, 2019.\n[22] Jones, O., R. Maillardet, and A. Robinson. Introduction to Scientific Programming and Simulation Using R. CRC Press, 2009.\n[23] Chang, Winston. R Graphics Cookbook. O’Reilly, 2012.\n[24] Zieffler, Andrew, et al. Comparing Groups Using R. Wiley, 2011.\n[25] DeGroot, Morris H., and Mark J. Schervish. Probability and Statistics. 4th ed. Pearson, 2012.\n[26] Hogg, Robert V., et al. Probability and Statistical Inference. 10th ed. Pearson, 2019.\n[27] Illowsky, Barbara, and Susan Dean. Introductory Statistics. 2nd ed. OpenStax, 2022.\n[28] Adhikari, Ani, et al. Computational and Inferential Thinking. 2nd ed., 2022.\n[29] Kleinberg, Jon, and Éva Tardos. Algorithm Design. Addison-Wesley.\n[30] Durrett, Rick. Essentials of Stochastic Processes. 2nd ed. Springer, 2012.\n[31] UCLA Statistics 100C handouts."
  },
  {
    "objectID": "projects_files/Courses.html#references",
    "href": "projects_files/Courses.html#references",
    "title": "Relevant Academic Coursework",
    "section": "References",
    "text": "References\n[1] Stewart, James. Calculus: Early Transcendentals. 8th ed. Cengage, 2015.\n[2] Larson, Ron. Elementary Linear Algebra. 8th ed. Cengage, 2017.\n[3] Epp, Susanna S. Discrete Mathematics with Applications. 5th ed. Cengage, 2019.\n[4] Zill, Dennis G. A First Course in Differential Equations with Modeling Applications. 11th ed. Cengage, 2018.\n[5] Friedberg, Stephen H., Arnold J. Insel, and Lawrence E. Spence. Linear Algebra. 5th ed. Pearson, 2024.\n[6] Ross, Sheldon M. A First Course in Probability. 10th ed. Pearson, 2019.\n[7] Burden, Richard L., J. Douglas Faires, and Annette M. Burden. Numerical Analysis. 10th ed. Cengage, 2015.\n[8] Rice, John A. Mathematical Statistics and Data Analysis. 3rd ed. Cengage, 2006.\n[9] Sheather, Simon J. A Modern Approach to Regression with R. Springer, 2009.\n[10] Kutner, Michael H., et al. Applied Linear Statistical Models. 5th ed. McGraw-Hill, 2005.\n[11] Abraham, B., and J. Ledolter. Introduction to Regression Modeling. Duxbury, 2006.\n[12] Montgomery, Douglas C. Design and Analysis of Experiments. 9th ed. Wiley, 2017.\n[13] Boyd, Stephen, and Lieven Vandenberghe. Convex Optimization. Cambridge UP, 2004.\n[14] James, Gareth, et al. An Introduction to Statistical Learning with R. 2nd ed., Springer, 2021.\n[15] Robert, Christian P., and George Casella. Introducing Monte Carlo Methods with R. Springer, 2010.\n[16] Bishop, Christopher M. Pattern Recognition and Machine Learning. Springer, 2006.\n[17] Durrett, Rick. Essentials of Stochastic Processes. 2nd ed. Springer, 2012.\n[18] Zhou, Qing. Course Notes for STATS 102B. UCLA.\n[19] Isaaks, Edward H., and R. Mohan Srivastava. An Introduction to Applied Geostatistics. Oxford UP, 1989.\n[20] UCLA Statistics 100C course handouts.\nhttp://www.stat.ucla.edu/~nchristo/statistics100C/\n[21] Wickham, Hadley. Advanced R. 2nd ed. Chapman & Hall/CRC, 2019.\n[22] Jones, O., R. Maillardet, and A. Robinson. Introduction to Scientific Programming and Simulation Using R. CRC Press, 2009.\n[23] Chang, Winston. R Graphics Cookbook. O’Reilly, 2012.\n[24] Zieffler, Andrew, et al. Comparing Groups Using R. Wiley, 2011.\n[25] DeGroot, Morris H., and Mark J. Schervish. Probability and Statistics. 4th ed. Pearson, 2012.\n[26] Hogg, Robert V., et al. Probability and Statistical Inference. 10th ed. Pearson, 2019.\n[27] Illowsky, Barbara, and Susan Dean. Introductory Statistics. 2nd ed. OpenStax, 2022.\n[28] Adhikari, Ani, et al. Computational and Inferential Thinking. 2nd ed., 2022.\n[29] Kleinberg, Jon, and Éva Tardos. Algorithm Design. Addison-Wesley.\n[30] Durrett, Rick. Essentials of Stochastic Processes. 2nd ed. Springer, 2012.\n[31] UCLA Statistics 100C handouts."
  },
  {
    "objectID": "Projects/stat140/Stat140.html#statistical-methods",
    "href": "Projects/stat140/Stat140.html#statistical-methods",
    "title": "How Do the Characteristics and Humanitarian Impacts of U.S. Counterterrorism Strikes Differ Between Somalia and Yemen?",
    "section": "Statistical Methods",
    "text": "Statistical Methods"
  },
  {
    "objectID": "Projects/stat140/Stat140.html#hypothesis-tests",
    "href": "Projects/stat140/Stat140.html#hypothesis-tests",
    "title": "How Do the Characteristics and Humanitarian Impacts of U.S. Counterterrorism Strikes Differ Between Somalia and Yemen?",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\nHypothesis 1: Civilian Harm Difference\n\\[\n\\begin{aligned}\nH_{0}: &\\ \\text{Drone strikes have the same civilian impact in Somalia and Yemen.} \\\\\nH_{1}: &\\ \\text{Drone strikes have different civilian impacts across the two regions.}\n\\end{aligned}\n\\]\nTo test whether Somalia and Yemen differ in civilian casualty rates, we estimate:\n\n\n\nVariable Definitions\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nCivilian casualties\nNumber of civilians reported killed in the strike (outcome variable).\n\n\nRegion\nCountry where the strike occurred (Somalia or Yemen).\n\n\nDrone\nIndicates whether the strike was carried out by a drone (1 = drone).\n\n\nUS confirmed\nWhether the strike was officially confirmed by the U.S. government.\n\n\nMinimum strikes\nMinimum number of strike events associated with the record.\n\n\nTotal killed\nMinimum number of total fatalities (civilians + militants).\n\n\n\n\n\n\n\n\n\n\nHypothesis 2: Drone Effectiveness Across Countries\n\\[\n\\begin{aligned}\nH_{0}: &\\ \\text{Drone use affects civilian casualties in the same way in both Somalia and Yemen.} \\\\\nH_{1}: &\\ \\text{Drone use affects civilian casualties differently across Somalia and Yemen.}\n\\end{aligned}\n\\]\nTo evaluate whether drones behave differently across countries, we added an interaction term (drone × region).\n\n\n\n\n\nHypothesis 3: Reporting Uncertainty Differenc\n\\[\n\\begin{aligned}\nH_{0}: &\\ \\text{Reporting uncertainty does not differ between Somalia and Yemen.} \\\\\nH_{1}: &\\ \\text{Reporting uncertainty differs between Somalia and Yemen.}\n\\end{aligned}\n\\]\n\nTo assess whether casualty reporting uncertainty differs between regions, we model the uncertainty metric. We modeled casualty reporting uncertainty (defined as max_killed - min_killed) region and strike characteristics as predictors.\n\\[\n\\text{Uncertainty in casualties} =\n\\text{Maximum killed} - \\text{Minimum killed}\n\\]"
  },
  {
    "objectID": "Projects/stat140/Stat140.html#test-for-hypothesis-2",
    "href": "Projects/stat140/Stat140.html#test-for-hypothesis-2",
    "title": "How Do the Characteristics and Humanitarian Impacts of U.S. Counterterrorism Strikes Differ Between Somalia and Yemen?",
    "section": "Test for Hypothesis 2",
    "text": "Test for Hypothesis 2"
  },
  {
    "objectID": "Projects/stat140/Stat140.html#test-for-hypothesis-3",
    "href": "Projects/stat140/Stat140.html#test-for-hypothesis-3",
    "title": "How Do the Characteristics and Humanitarian Impacts of U.S. Counterterrorism Strikes Differ Between Somalia and Yemen?",
    "section": "Test for Hypothesis 3",
    "text": "Test for Hypothesis 3\n\n##################################\n## Hypothesis 3: Reporting Uncertainty ##\n##################################\n# H0: The level of uncertainty in casualty reporting does not differ between Somalia and Yemen.\n# H1: It does differ.\n# Outcome: uncertainty_killed\n\nmodel_h3 &lt;- glm.nb(\n  uncertainty_killed ~ region + drone + us_confirmed + min_strikes,\n  data = combined_model\n)\n\nsummary(model_h3)\n\n\nCall:\nglm.nb(formula = uncertainty_killed ~ region + drone + us_confirmed + \n    min_strikes, data = combined_model, init.theta = 0.2157180244, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   0.64763    0.30943   2.093  0.03635 * \nregionYemen  -0.16294    0.26446  -0.616  0.53781   \ndrone         0.61827    0.24966   2.476  0.01327 * \nus_confirmed -0.69343    0.25255  -2.746  0.00604 **\nmin_strikes   0.08520    0.05284   1.612  0.10687   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.2157) family taken to be 1)\n\n    Null deviance: 400.69  on 513  degrees of freedom\nResidual deviance: 385.48  on 509  degrees of freedom\nAIC: 1571.1\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2157 \n          Std. Err.:  0.0218 \n\n 2 x log-likelihood:  -1559.1480 \n\ntidy(model_h3, exponentiate = TRUE, conf.int = TRUE)\n\n# A tibble: 5 × 7\n  term         estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1.91     0.309      2.09  0.0363     1.13      3.34 \n2 regionYemen     0.850    0.264     -0.616 0.538      0.536     1.33 \n3 drone           1.86     0.250      2.48  0.0133     1.18      2.91 \n4 us_confirmed    0.500    0.253     -2.75  0.00604    0.305     0.793\n5 min_strikes     1.09     0.0528     1.61  0.107      0.990     1.28 \n\n###############################################\n## 5. Optional: quick descriptive checks\n###############################################\n\n# Mean civilian casualties by region & drone status\ncombined_model %&gt;%\n  group_by(region, drone) %&gt;%\n  summarise(\n    mean_civilian_casualties = mean(civilian_casualties, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# A tibble: 4 × 4\n  region  drone mean_civilian_casualties     n\n  &lt;fct&gt;   &lt;int&gt;                    &lt;dbl&gt; &lt;int&gt;\n1 Somalia     0                   0.0753   146\n2 Somalia     1                   0.114     44\n3 Yemen       0                   1.38      76\n4 Yemen       1                   0.411    248\n\n# Mean uncertainty by region\ncombined_model %&gt;%\n  group_by(region) %&gt;%\n  summarise(\n    mean_uncertainty = mean(uncertainty_killed, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 3\n  region  mean_uncertainty     n\n  &lt;fct&gt;              &lt;dbl&gt; &lt;int&gt;\n1 Somalia             1.56   190\n2 Yemen               1.84   324\n\n\n\nResult for Hypothesis 3: Reporting Uncertainty\nReporting uncertainty does not differ between regions (Yemen coef = −0.16, p = 0.538).\nHowever, drone strikes show higher uncertainty (coef = 0.62, IRR = 1.86, p = 0.013), while confirmed U.S. strikes show lower uncertainty (coef = −0.69, IRR = 0.50, p = 0.006).\nConclusion: No regional difference in uncertainty → H3 not supported, though uncertainty varies by strike type."
  },
  {
    "objectID": "Projects/stat140/Stat140.html#visualizations",
    "href": "Projects/stat140/Stat140.html#visualizations",
    "title": "How Do the Characteristics and Humanitarian Impacts of U.S. Counterterrorism Strikes Differ Between Somalia and Yemen?",
    "section": "Visualizations",
    "text": "Visualizations"
  },
  {
    "objectID": "Projects/stat140/Stat140.html#hypothesis-test-1",
    "href": "Projects/stat140/Stat140.html#hypothesis-test-1",
    "title": "How Do the Characteristics and Humanitarian Impacts of U.S. Counterterrorism Strikes Differ Between Somalia and Yemen?",
    "section": "Hypothesis Test 1",
    "text": "Hypothesis Test 1\n\\[\n\\begin{aligned}\nH_{0}: &\\ \\text{Drone strikes have the same civilian impact in Somalia and Yemen.} \\\\\nH_{1}: &\\ \\text{Drone strikes have different civilian impacts across the two regions.}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "Projects/stat140/Stat140.html#hypothesis-test-2",
    "href": "Projects/stat140/Stat140.html#hypothesis-test-2",
    "title": "How Do the Characteristics and Humanitarian Impacts of U.S. Counterterrorism Strikes Differ Between Somalia and Yemen?",
    "section": "Hypothesis Test 2",
    "text": "Hypothesis Test 2"
  },
  {
    "objectID": "Projects/stat140/Stat140.html#hypothesis-test-3",
    "href": "Projects/stat140/Stat140.html#hypothesis-test-3",
    "title": "How Do the Characteristics and Humanitarian Impacts of U.S. Counterterrorism Strikes Differ Between Somalia and Yemen?",
    "section": "Hypothesis Test 3",
    "text": "Hypothesis Test 3\n\\[\n\\begin{aligned}\nH_{0}: &\\ \\text{Reporting uncertainty does not differ between Somalia and Yemen.} \\\\\nH_{1}: &\\ \\text{Reporting uncertainty differs between Somalia and Yemen.}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "Projects/Mag/Mag.html",
    "href": "Projects/Mag/Mag.html",
    "title": "Magnetic nanocomposites for biomedical applications",
    "section": "",
    "text": "Abstract\n\n\n\n\nTissue engineering and regenerative medicine have solved numerous problems related to the repair and regeneration of damaged organs and tissues arising from aging, illnesses, and injuries. Nanotechnology has further aided tissue regeneration science and has provided outstanding opportunities to help disease diagnosis as well as treat damaged tissues. Based on the most recent findings, magnetic nanostructures (MNSs), in particular, have emerged as promising materials for detecting, directing, and supporting tissue regeneration. There have been many reports concerning the role of these nano-building blocks in the regeneration of both soft and hard tissues, but the subject has not been extensively reviewed. Here, we review, classify, and discuss various synthesis strategies for novel MNSs used in medicine. Advanced applications of magnetic nanocomposites (MG-NCs), specifically magnetic nanostructures, are further systematically reviewed. In addition, the scientific and technical aspects of MG-NC used in medicine are discussed considering the requirements for the field. In summary, this review highlights the numerous opportunities and challenges associated with the use of MG-NCs as smart nanocomposites (NCs) in tissue engineering and regenerative medicine.\n\n\n\n\n\n\n\n View Published Article"
  },
  {
    "objectID": "Projects/CellTypes/CellTypes.html#section",
    "href": "Projects/CellTypes/CellTypes.html#section",
    "title": "Cell Types: Origin and Function",
    "section": "",
    "text": "Abstract\n\n\n\nCells are essential components of biological systems and are involved in various activities, such as interacting with the surrounding environment. Researchers are taking advantage of naturally derived cell membranes to enhance the bio-interfacing capabilities of nanoparticles. The use of cell membrane-coating technology represents an exciting breakthrough in the field of nanomedicine. This technique has the potential to enhance the ability of nanoparticles to interact with biological systems and deliver drugs or therapeutic agents more effectively in clinical settings. This chapter provides a comprehensive overview of the different types of cells used for membrane coating of nanoparticles, including red blood cells, platelets, white blood cells, stem cells, cancer cells, hybrid cells, and other specialized cell types like beta cells, endothelial cells, fibroblast cells, and bacterial cells. The chapter delves into the origin and function of each of these cell types and highlights the potential applications of cell membrane-coating technology for each.\n\n View Book Chapter"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Keivan Bolouri",
    "section": "",
    "text": "Welcome to my page!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Keivan Bolouri",
    "section": "About me",
    "text": "About me\nHello! My name is Keivan Bolouri. I am passionate about technology, data, and continuous learning.\nI enjoy building projects, exploring new tools, and applying what I learn to real-world problems."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Keivan Bolouri",
    "section": "Education",
    "text": "Education\n\nUniversity of California, Los Angeles (UCLA)\n\nI’m currently pursuing a B.S. in Statistics and Data Science at UCLA, where I focus on turning complex data into clear, actionable insights. Alongside core training in statistical modeling and algorithmic thinking, I’m especially interested in statistical genetics and computational biology—fields that let me explore how data can reveal the patterns and mechanisms behind biological systems.\n\n\n\nOutside of academics, I enjoy hiking, running, and staying active outdoors.\nFeel free to explore my links above to learn more about my work and interests. ✨"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Abou",
    "section": "",
    "text": "About this site\n\n1 + 778\n\n[1] 779\n\n\n\nrunif(100)\n\n  [1] 0.60450323 0.26939761 0.75518378 0.70152145 0.90652913 0.80352336\n  [7] 0.29800393 0.54475586 0.83846580 0.94991008 0.02380019 0.33148395\n [13] 0.97815538 0.37288285 0.64277301 0.81568098 0.80343214 0.01050563\n [19] 0.55160804 0.44645847 0.22588771 0.27179749 0.31839931 0.92699534\n [25] 0.50695460 0.65929622 0.60419510 0.57366298 0.23201691 0.31186663\n [31] 0.23627621 0.65163347 0.85905099 0.02563029 0.31984957 0.27710044\n [37] 0.28531048 0.92005824 0.63537660 0.29936726 0.72983740 0.02866435\n [43] 0.56812685 0.91810867 0.74594929 0.76446033 0.29782261 0.49274161\n [49] 0.34206005 0.48386200 0.90350366 0.56106893 0.39604104 0.90153925\n [55] 0.37691143 0.57484579 0.51654046 0.48193785 0.24614207 0.10324070\n [61] 0.59908440 0.38550057 0.53910121 0.89061274 0.98342933 0.64865640\n [67] 0.37571939 0.55550558 0.51402009 0.37069886 0.59876499 0.14377437\n [73] 0.80835799 0.56379443 0.27255667 0.44588201 0.53279457 0.92907784\n [79] 0.74545370 0.58494085 0.43175845 0.72305029 0.93790646 0.21530477\n [85] 0.67324506 0.39707932 0.19369650 0.96097775 0.20663721 0.80979981\n [91] 0.28409360 0.08398440 0.40670273 0.02747251 0.65488462 0.98550051\n [97] 0.64277267 0.38869640 0.35678593 0.13871013\n\n\n\nset.seed(123)\nplot(1:20, runif(20))\n\n\n\n\n\n\n\n\n\nplot(1:40,runif(40))\n\n\n\n\n\n\n\n\n\nx &lt;- seq(0, 2*pi, length.out = 100)  # 100 points from 0 to 2π\ny &lt;- sin(x)                          # sine of each x\nplot(x, y, type = \"l\")               # 'l' means line plot\n\n\n\n\n\n\n\nplot(x, y, \n     type = \"l\", \n     col = \"blue\", \n     lwd = 2,\n     main = \"Plot of sin(x)\",\n     xlab = \"x (radians)\",\n     ylab = \"sin(x)\")\nlines(x, cos(x), col = \"red\", lwd = 2)\nlegend(\"topright\", legend = c(\"sin(x)\", \"cos(x)\"),\n       col = c(\"blue\", \"red\"), lwd = 2)"
  },
  {
    "objectID": "Courses.html#coursework-completed-and-in-progress",
    "href": "Courses.html#coursework-completed-and-in-progress",
    "title": "Relevant Academic Coursework",
    "section": "Coursework Completed and In-Progress",
    "text": "Coursework Completed and In-Progress\n\nUpper Division and Graduate-Level Mathematics & Statistics Courses\n\n\n\n\n\n\n\n\n\n\nNo.\nCourse (Number & Title)\nTerm\nTextbook Ref\n\n\n\n\n1\nMATH 115A – Linear Algebra\nFall 2024\n[5]\n\n\n2\nSTATS 100A – Introduction to Probability\nFall 2024\n[25], [26]\n\n\n3\nMATH 151A – Applied Numerical Methods\nWinter 2025\n[7]\n\n\n4\nSTATS 100B – Mathematical Statistics\nWinter 2025\n[8]\n\n\n5\nSTATS 101A – Data Analysis & Regression\nWinter 2025\n[9], [10]\n\n\n6\nSTATS 100C – Linear Models\nSpring 2025\n[11], [31]\n\n\n7\nSTATS 101B – Design & Analysis of Experiments\nSpring 2025\n[12]\n\n\n8\nSTATS 102A – Intro to Computational Statistics (R)\nSummer 2025\n[21], [22], [23], [24]\n\n\n9\nMATH 164 – Optimization\nFall 2025\n[13]\n\n\n10\nSTATS 101C – Statistical Models & Data Mining\nFall 2025\n[14]\n\n\n11\nSTATS 102C – Monte Carlo Methods\nFall 2025\n[15]\n\n\n12\nMATH 156 – Machine Learning\nWinter 2026\n[16]\n\n\n13\nMATH 171 – Stochastic Processes\nWinter 2026\n[30]\n\n\n14\nSTATS 102B – Computation & Optimization for Statistics\nSpring 2026\n[18]\n\n\n15\nMATH 182 – Algorithms\nSpring 2026\n[29]"
  },
  {
    "objectID": "Courses.html#lower-division-mathematics-statistics-courses",
    "href": "Courses.html#lower-division-mathematics-statistics-courses",
    "title": "Relevant Academic Coursework",
    "section": "Lower Division Mathematics & Statistics Courses",
    "text": "Lower Division Mathematics & Statistics Courses\n\n\n\n\n\n\n\n\n\n\nNo.\nCourse (Number & Title)\nTerm\nTextbook Ref\n\n\n\n\n1\nMATH-227 – Statistics\nWinter 2022\n[27]\n\n\n2\nMATH-229 – Statistics for Data Science\nSummer 2024\n[28]\n\n\n3\nMATH 265 – Calculus I\nWinter 2022\n[1]\n\n\n4\nMATH 266 – Calculus II\nSpring 2022\n[1]\n\n\n5\nMATH 267 – Calculus III\nFall 2022\n[1]\n\n\n6\nMATH 270 – Linear Algebra\nFall 2022\n[2]\n\n\n7\nMATH 272 – Methods of Discrete Mathematics\nFall 2023\n[3]\n\n\n8\nMATH 275 – Ordinary Differential Equations\nSpring 2023\n[4]\n\n\n\n\n\n\n\n\n\n\nNoteShow Textbook References\n\n\n\n\n\nReferences\n[1] Stewart, James. Calculus: Early Transcendentals. 8th ed. Cengage, 2015.\n[2] Larson, Ron. Elementary Linear Algebra. 8th ed. Cengage, 2017.\n[3] Epp, Susanna S. Discrete Mathematics with Applications. 5th ed. Cengage, 2019.\n[4] Zill, Dennis G. A First Course in Differential Equations with Modeling Applications. 11th ed. Cengage, 2018.\n[5] Friedberg, Stephen H., Arnold J. Insel, and Lawrence E. Spence. Linear Algebra. 5th ed. Pearson, 2024.\n[6] Ross, Sheldon M. A First Course in Probability. 10th ed. Pearson, 2019.\n[7] Burden, Richard L., J. Douglas Faires, and Annette M. Burden. Numerical Analysis. 10th ed. Cengage, 2015.\n[8] Rice, John A. Mathematical Statistics and Data Analysis. 3rd ed. Cengage, 2006.\n[9] Sheather, Simon J. A Modern Approach to Regression with R. Springer, 2009.\n[10] Kutner, Michael H., et al. Applied Linear Statistical Models. 5th ed. McGraw-Hill, 2005.\n[11] Abraham, B., and J. Ledolter. Introduction to Regression Modeling. Duxbury, 2006.\n[12] Montgomery, Douglas C. Design and Analysis of Experiments. 9th ed. Wiley, 2017.\n[13] Boyd, Stephen, and Lieven Vandenberghe. Convex Optimization. Cambridge UP, 2004.\n[14] James, Gareth, et al. An Introduction to Statistical Learning with R. 2nd ed., Springer, 2021.\n[15] Robert, Christian P., and George Casella. Introducing Monte Carlo Methods with R. Springer, 2010.\n[16] Bishop, Christopher M. Pattern Recognition and Machine Learning. Springer, 2006.\n[17] Durrett, Rick. Essentials of Stochastic Processes. 2nd ed. Springer, 2012.\n[18] Zhou, Qing. Course Notes for STATS 102B. UCLA.\n[19] Isaaks, Edward H., and R. Mohan Srivastava. An Introduction to Applied Geostatistics. Oxford UP, 1989.\n[20] UCLA Statistics 100C course handouts.\nhttp://www.stat.ucla.edu/~nchristo/statistics100C/\n[21] Wickham, Hadley. Advanced R. 2nd ed. Chapman & Hall/CRC, 2019.\n[22] Jones, O., R. Maillardet, and A. Robinson. Introduction to Scientific Programming and Simulation Using R. CRC Press, 2009.\n[23] Chang, Winston. R Graphics Cookbook. O’Reilly, 2012.\n[24] Zieffler, Andrew, et al. Comparing Groups Using R. Wiley, 2011.\n[25] DeGroot, Morris H., and Mark J. Schervish. Probability and Statistics. 4th ed. Pearson, 2012.\n[26] Hogg, Robert V., et al. Probability and Statistical Inference. 10th ed. Pearson, 2019.\n[27] Illowsky, Barbara, and Susan Dean. Introductory Statistics. 2nd ed. OpenStax, 2022.\n[28] Adhikari, Ani, et al. Computational and Inferential Thinking. 2nd ed., 2022.\n[29] Kleinberg, Jon, and Éva Tardos. Algorithm Design. Addison-Wesley.\n[30] Durrett, Rick. Essentials of Stochastic Processes. 2nd ed. Springer, 2012.\n[31] UCLA Statistics 100C handouts."
  },
  {
    "objectID": "Courses.html#references",
    "href": "Courses.html#references",
    "title": "Relevant Academic Coursework",
    "section": "References",
    "text": "References\n[1] Stewart, James. Calculus: Early Transcendentals. 8th ed. Cengage, 2015.\n[2] Larson, Ron. Elementary Linear Algebra. 8th ed. Cengage, 2017.\n[3] Epp, Susanna S. Discrete Mathematics with Applications. 5th ed. Cengage, 2019.\n[4] Zill, Dennis G. A First Course in Differential Equations with Modeling Applications. 11th ed. Cengage, 2018.\n[5] Friedberg, Stephen H., Arnold J. Insel, and Lawrence E. Spence. Linear Algebra. 5th ed. Pearson, 2024.\n[6] Ross, Sheldon M. A First Course in Probability. 10th ed. Pearson, 2019.\n[7] Burden, Richard L., J. Douglas Faires, and Annette M. Burden. Numerical Analysis. 10th ed. Cengage, 2015.\n[8] Rice, John A. Mathematical Statistics and Data Analysis. 3rd ed. Cengage, 2006.\n[9] Sheather, Simon J. A Modern Approach to Regression with R. Springer, 2009.\n[10] Kutner, Michael H., et al. Applied Linear Statistical Models. 5th ed. McGraw-Hill, 2005.\n[11] Abraham, B., and J. Ledolter. Introduction to Regression Modeling. Duxbury, 2006.\n[12] Montgomery, Douglas C. Design and Analysis of Experiments. 9th ed. Wiley, 2017.\n[13] Boyd, Stephen, and Lieven Vandenberghe. Convex Optimization. Cambridge UP, 2004.\n[14] James, Gareth, et al. An Introduction to Statistical Learning with R. 2nd ed., Springer, 2021.\n[15] Robert, Christian P., and George Casella. Introducing Monte Carlo Methods with R. Springer, 2010.\n[16] Bishop, Christopher M. Pattern Recognition and Machine Learning. Springer, 2006.\n[17] Durrett, Rick. Essentials of Stochastic Processes. 2nd ed. Springer, 2012.\n[18] Zhou, Qing. Course Notes for STATS 102B. UCLA.\n[19] Isaaks, Edward H., and R. Mohan Srivastava. An Introduction to Applied Geostatistics. Oxford UP, 1989.\n[20] UCLA Statistics 100C course handouts.\nhttp://www.stat.ucla.edu/~nchristo/statistics100C/\n[21] Wickham, Hadley. Advanced R. 2nd ed. Chapman & Hall/CRC, 2019.\n[22] Jones, O., R. Maillardet, and A. Robinson. Introduction to Scientific Programming and Simulation Using R. CRC Press, 2009.\n[23] Chang, Winston. R Graphics Cookbook. O’Reilly, 2012.\n[24] Zieffler, Andrew, et al. Comparing Groups Using R. Wiley, 2011.\n[25] DeGroot, Morris H., and Mark J. Schervish. Probability and Statistics. 4th ed. Pearson, 2012.\n[26] Hogg, Robert V., et al. Probability and Statistical Inference. 10th ed. Pearson, 2019.\n[27] Illowsky, Barbara, and Susan Dean. Introductory Statistics. 2nd ed. OpenStax, 2022.\n[28] Adhikari, Ani, et al. Computational and Inferential Thinking. 2nd ed., 2022.\n[29] Kleinberg, Jon, and Éva Tardos. Algorithm Design. Addison-Wesley.\n[30] Durrett, Rick. Essentials of Stochastic Processes. 2nd ed. Springer, 2012.\n[31] UCLA Statistics 100C handouts."
  },
  {
    "objectID": "Projects/Add/Ad.html#section",
    "href": "Projects/Add/Ad.html#section",
    "title": "Advances in electroconductive polymers for biomedical sector: structure and properties",
    "section": "",
    "text": "Abstract\n\nThis review examines the synthesis, properties, and broad-spectrum applications of electroconductive polymers (ECPs), including polyaniline, polypyrrole, polythiophene, polyphenylene, and polyacetylene. These polymers exhibit high electrical conductivity, versatility in fabrication, and compatibility with various functionalization techniques, making them particularly attractive for diverse applications. While ECPs have traditionally been used in sensors, actuators, and energy storage systems, their utility extends much further, most notably to the realm of biomedical applications. The review meticulously explores the synthesis techniques of ECPs, shedding light on both chemical and electrochemical methods, and the pivotal role that dopants and polymerization techniques play in shaping the properties of the resultant polymers. Apart from discussing the conventional applications of ECPs, the review devotes substantial attention to their groundbreaking biomedical applications, like tissue engineering, medical implants, and the creation of interfaces with biological tissues. It also underscores the future trajectory of ECP research, emphasizing the development of innovative materials and fabrication methodologies for more advanced applications. With this holistic analysis of the field, the review seeks to enhance readers’’ understanding of the intrinsic properties, structural complexities, and fabrication nuances of ECPs, and inspire continued research and development in this fascinating and consequential domain of materials science.\n\n View Published Article"
  },
  {
    "objectID": "Projects/Harness/Harness.html",
    "href": "Projects/Harness/Harness.html",
    "title": "Harnessing the Power of Electroconductive Polymers for Breakthroughs in Tissue Engineering and Regenerative Medicine",
    "section": "",
    "text": "Abstract\n\nElectroconductive polymers (ECPs) have garnered increasing attention in the realms of tissue engineering and regenerative medicine due to their unique physicochemical properties, including their ability to conduct electrical signals. These polymers, with inherent conductivity mirroring that of native tissues, present a promising platform for scaffolds that can modulate cell behavior and tissue formation through electrical stimulation. The biocompatibility, tunable conductivity, and topographical features of ECPs enhance cellular adhesion, proliferation, and differentiation. Furthermore, their electrical properties have been shown to augment nerve regeneration, cardiac tissue repair, and musculoskeletal tissue formation. Combined with other biomaterials or biological molecules, ECP-based composites exhibit synergistic effects, promoting enhanced tissue regeneration. Moreover, the integration of ECPs with cutting-edge technologies such as 3D printing and microfluidics propels the design of sophisticated constructs for tissue engineering applications. This paper concludes with the challenges faced in the clinical translation of ECP-based scaffolds and provides perspectives on the future trajectory of ECPs in regenerative medicine. The synthesis of ECPs with emerging biotechnologies has the potential to revolutionize treatments, bridging the gap between traditional regenerative approaches and sophisticated bioelectronic remedies.\n\n View Published Article"
  },
  {
    "objectID": "Projects/Metal/Metal.html",
    "href": "Projects/Metal/Metal.html",
    "title": "Metal-Organic Frameworks in Bone Regeneration",
    "section": "",
    "text": "Abstract\n\nThe occurrence of traumatic bone defects caused by accidents, diseases, and surgeries has become increasingly common. Consequently, there has been a noticeable increase in the overall number of bone defects reported. Treating bone defects is characterized by long treatment periods, high costs, and unpredictable outcomes, often accompanied by complications like infections and bone discontinuity. Consequently, this situation significantly impacts the physical, mental, and financial well-being of patients and poses a challenge to orthopedic surgeons. This has piqued a considerable interest in the realm of bone therapy and repair. Materials other than autogenous bone have not yet achieved the ability to offer ideal biocompatibility, osteogenesis, osteoconductivity, and osteoinduction properties simultaneously. Moreover, the scarcity of autologous bone sources has necessitated the search for new replacement materials. Metal-organic frameworks (MOFs) represent a novel class of functional materials that have gained extensive attention in the biomedical field in recent years. This is attributed to their porous nature, large specific surface area, tailorable chemistry, and their potential for drug loading. As bone treatment and repair research progresses, more investigators are exploring the potential of using MOFs for bone therapy and repair applications. Taking all these aspects into account, this chapter summarizes the current utilization of MOFs in bone therapy and regeneration, while also providing an outlook on the potential prospects of MOFs in this field.\n\n\n View Book Chapter"
  },
  {
    "objectID": "Projects/SkinCancer/Mag.html",
    "href": "Projects/SkinCancer/Mag.html",
    "title": "Magnetic nanocomposites for biomedical applications",
    "section": "",
    "text": "Abstract\n\n\n\n\nTissue engineering and regenerative medicine have solved numerous problems related to the repair and regeneration of damaged organs and tissues arising from aging, illnesses, and injuries. Nanotechnology has further aided tissue regeneration science and has provided outstanding opportunities to help disease diagnosis as well as treat damaged tissues. Based on the most recent findings, magnetic nanostructures (MNSs), in particular, have emerged as promising materials for detecting, directing, and supporting tissue regeneration. There have been many reports concerning the role of these nano-building blocks in the regeneration of both soft and hard tissues, but the subject has not been extensively reviewed. Here, we review, classify, and discuss various synthesis strategies for novel MNSs used in medicine. Advanced applications of magnetic nanocomposites (MG-NCs), specifically magnetic nanostructures, are further systematically reviewed. In addition, the scientific and technical aspects of MG-NC used in medicine are discussed considering the requirements for the field. In summary, this review highlights the numerous opportunities and challenges associated with the use of MG-NCs as smart nanocomposites (NCs) in tissue engineering and regenerative medicine.\n\n\n\n\n\n\n\n View Published Article"
  },
  {
    "objectID": "Projects/SkinCancer/SkinCancer.html",
    "href": "Projects/SkinCancer/SkinCancer.html",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "",
    "text": "⬇ Download PDF"
  },
  {
    "objectID": "Projects/SkinCancer/SkinCancer.html#exploratory-data-analysis",
    "href": "Projects/SkinCancer/SkinCancer.html#exploratory-data-analysis",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWe began by examining the overall structure and distributions in the training dataset. The response variable (Cancer) is binary (Benign or Malignant), and we observed a slight class imbalance: benign lesions were more frequent than malignant lesions in the training data. This imbalance means that a naive classifier that predicts all cases as benign would achieve a non-trivial accuracy, so careful model evaluation is necessary. We also inspected the distribution of key predictors. For example, the average age of patients with malignant lesions appeared higher than that of patients with benign lesions, consistent with the fact that skin cancer risk increases with age. We explored categorical risk factors as well: certain exposure-related factors (such as indicators of high UV exposure or tanning habits) were somewhat more prevalent in malignant cases, though there was significant overlap between the benign and malignant groups for most individual predictors. Overall, no single predictor showed a dramatic separation between malignant and benign lesions in isolation, suggesting that multiple factors in combination would be needed for effective prediction. In addition, we checked for missing data and outliers during the exploratory phase. We found that the dataset’s overall quality was high, with only a moderate amount of missing values (on the order of 7–8% missing per predictor). There was no evidence of extreme outliers that would require removal or transformation beyond standardization. The presence of some missing values and the lack of obvious one-variable predictors of cancer underlined the need for a robust modeling approach with proper data preprocessing, which we implemented as described below."
  },
  {
    "objectID": "Projects/SkinCancer/SkinCancer.html#data-cleaning-and-missing-values",
    "href": "Projects/SkinCancer/SkinCancer.html#data-cleaning-and-missing-values",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Data Cleaning and Missing Values",
    "text": "Data Cleaning and Missing Values\nAfter completing the exploratory analysis, we examined the dataset for missing values. We found that the overall data quality was relatively high, with most predictors containing approximately 7–8% missing values. No predictor exceeded 10% missingness, so eliminating variables or observations would have resulted in unnecessary data loss. To address missing values in a consistent manner, we applied the following imputation strategy: Numerical variables were imputed using the median, which is robust to outliers. Categorical variables were imputed using the most frequent category (mode). This approach allowed us to preserve all 50,000 observations in the training dataset while ensuring that the data were suitable for modeling. After imputation, no missing values remained in the predictors used for analysis.\n\n\n\nTable 1: Top ten variables with the highest proportion of missing values.\n\n\n\n\n\n\n\n\nVariable\nMissing (%)\n\n\n\n\nvitamin_d_supplement\n8.3\n\n\nphone_brand\n8.2\n\n\nskin_tone\n8.2\n\n\nsunscreen_spf\n8.2\n\n\npreferred_shoe_type\n8.2\n\n\nsunscreen_freq\n8.2\n\n\nresidence_lat\n8.2\n\n\ncommute_minutes\n8.1\n\n\nnear_high_power_cables\n8.1\n\n\nincome\n8.1\n\n\n\n (a) Training set variables.\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMissing (%)\n\n\n\n\nresidence_lon\n8.5\n\n\nphone_brand\n8.4\n\n\npreferred_shoe_type\n8.3\n\n\nuses_smartwatch\n8.3\n\n\nskin_tone\n8.3\n\n\nsmoking_status\n8.2\n\n\nincome\n8.2\n\n\nnear_high_power_cables\n8.2\n\n\nlesion_color\n8.2\n\n\nskin_photosensitivity\n8.1\n\n\n\n (b) Test set variables.\n\n\n\n\n\n\n\n\n\n\n\nUsing these association measures, all predictors were ranked from strongest to weakest. Variables with very weak association to the response variable were discarded, while stronger predictors were retained. This process reduced the dataset from 51 variables to 35 total variables, including the response variable. The final set consisted of 20 numerical predictors and 14 categorical predictors, while maintaining all original observations. This variable selection process helped simplify the modeling stage, reduced multicollinearity, and improved model interpretability, while retaining the most informative predictors for classification."
  },
  {
    "objectID": "Projects/SkinCancer/SkinCancer.html#variable-selection",
    "href": "Projects/SkinCancer/SkinCancer.html#variable-selection",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Variable Selection",
    "text": "Variable Selection\nGiven the large number of predictors, variable selection was necessary to reduce dimensionality and eliminate weak predictors that could introduce noise and increase the risk of overfitting. For categorical predictors, we used Cramér’s V to measure the strength of association between each categorical variable and the cancer outcome. Cramér’s V is based on the chi-squared statistic and produces values between 0 and 1, with larger values indicating stronger association. Here is a sample of the first 10 variables: For numerical predictors, we computed the point-biserial correlation, which measures the strength of association between a continuous predictor and a binary response variable. Numerical variables were ranked based on the absolute value of this correlation.\n\n\n\nTop 10 Categorical Predictors Ranked by Cramér’s V (Training Data)\n\n\nVariable\nCramér’s V\n\n\n\n\nfamily_history\n0.104\n\n\nskin_tone\n0.087\n\n\nsunscreen_freq\n0.066\n\n\nimmunosuppressed\n0.060\n\n\noccupation\n0.045\n\n\ntanning_bed_use\n0.041\n\n\nclothing_protection\n0.036\n\n\nskin_photosensitivity\n0.033\n\n\nhat_use\n0.028\n\n\nlesion_location\n0.011\n\n\n\n\n\n\nTop 10 Numerical Predictors Ranked by Point-Biserial Correlation (Training Data)\n\n\nVariable\nPoint-Biserial Correlation\n\n\n\n\nage\n0.1355\n\n\navg_daily_uv\n0.0599\n\n\nnumber_of_lesions\n0.0513\n\n\nsunburns_last_year\n0.0457\n\n\noutdoor_job\n0.0451\n\n\nlesion_size_mm\n0.0223\n\n\nsunscreen_spf\n-0.0174\n\n\nyears_lived_at_address\n0.0103\n\n\nincome\n0.0088\n\n\ndesk_height_cm\n0.0083\n\n\n\n\n\n[1] 35\n\n\n\n\n\n\nFinal Selected Predictors After Association-Based Variable Screening\n\n\nNo\nType\nVariable\nSelection Criterion\n\n\n\n\n1\nCategorical\nfamily_history\nCramér’s V &gt; 0.05\n\n\n2\nCategorical\nskin_tone\nCramér’s V &gt; 0.05\n\n\n3\nCategorical\nsunscreen_freq\nCramér’s V &gt; 0.05\n\n\n4\nCategorical\nimmunosuppressed\nCramér’s V &gt; 0.05\n\n\n5\nCategorical\noccupation\nCramér’s V &gt; 0.05\n\n\n6\nCategorical\ntanning_bed_use\nCramér’s V &gt; 0.05\n\n\n7\nCategorical\nclothing_protection\nCramér’s V &gt; 0.05\n\n\n8\nCategorical\nskin_photosensitivity\nCramér’s V &gt; 0.05\n\n\n9\nCategorical\nhat_use\nCramér’s V &gt; 0.05\n\n\n10\nCategorical\nlesion_location\nCramér’s V &gt; 0.05\n\n\n11\nCategorical\nfavorite_cuisine\nCramér’s V &gt; 0.05\n\n\n12\nCategorical\nmusic_genre\nCramér’s V &gt; 0.05\n\n\n13\nCategorical\nlesion_color\nCramér’s V &gt; 0.05\n\n\n14\nCategorical\nsunscreen_brand\nCramér’s V &gt; 0.05\n\n\n1\nNumerical\nage\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n2\nNumerical\navg_daily_uv\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n3\nNumerical\nnumber_of_lesions\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n4\nNumerical\nsunburns_last_year\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n5\nNumerical\noutdoor_job\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n6\nNumerical\nlesion_size_mm\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n7\nNumerical\nsunscreen_spf\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n8\nNumerical\nyears_lived_at_address\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n9\nNumerical\nincome\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n10\nNumerical\ndesk_height_cm\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n11\nNumerical\nresidence_lon\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n12\nNumerical\nexercise_freq_per_week\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n13\nNumerical\nzip_code_last_digit\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n14\nNumerical\ndistance_from_beach_km\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n15\nNumerical\nalcohol_drinks_per_week\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n16\nNumerical\nBMI\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n17\nNumerical\ncommute_minutes\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n18\nNumerical\nfrequency_doctor_visits_per_year\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n19\nNumerical\nresidence_lat\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n20\nNumerical\nmonthly_screen_time_minutes\nAbs. Point-Biserial Corr. &gt; 0.02"
  },
  {
    "objectID": "Projects/SkinCancer/Example.knit.html",
    "href": "Projects/SkinCancer/Example.knit.html",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "",
    "text": "This project develops and evaluates statistical learning models to classify skin lesions as benign or malignant using structured (non-image) predictors. Using a skin cancer dataset with 50,000 training observations and 20,000 test observations, we performed exploratory analysis, handled missing values via median (numeric) and mode (categorical) imputation, standardized numerical features, and reduced dimensionality through association-based feature screening. We then trained and compared a baseline logistic regression model with regularized variants (LASSO and elastic net logistic regression), tuning hyperparameters via cross-validation to balance generalization and performance under slight class imbalance.\nThe final model was an elastic net logistic regression with mixing parameter \\(\\alpha = 0.7\\) and 34 predictors, selected based on leaderboard performance. On the held-out test set, the model achieved 60.508% accuracy, outperforming naive baselines but remaining modest for a diagnostic setting. Overall, results suggest that structured demographic, behavioral, environmental, and clinical variables provide limited discriminative signal without lesion imaging features, highlighting both the potential utility of tabular predictors and the limitations of relying on them alone for skin cancer detection."
  },
  {
    "objectID": "Projects/SkinCancer/Example.knit.html#exploratory-data-analysis",
    "href": "Projects/SkinCancer/Example.knit.html#exploratory-data-analysis",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWe began by examining the overall structure and distributions in the training dataset. The response variable (Cancer) is binary (Benign or Malignant), and we observed a slight class imbalance: benign lesions were more frequent than malignant lesions in the training data. This imbalance means that a naive classifier that predicts all cases as benign would achieve a non-trivial accuracy, so careful model evaluation is necessary. We also inspected the distribution of key predictors. For example, the average age of patients with malignant lesions appeared higher than that of patients with benign lesions, consistent with the fact that skin cancer risk increases with age. We explored categorical risk factors as well: certain exposure-related factors (such as indicators of high UV exposure or tanning habits) were somewhat more prevalent in malignant cases, though there was significant overlap between the benign and malignant groups for most individual predictors. Overall, no single predictor showed a dramatic separation between malignant and benign lesions in isolation, suggesting that multiple factors in combination would be needed for effective prediction. In addition, we checked for missing data and outliers during the exploratory phase. We found that the dataset’s overall quality was high, with only a moderate amount of missing values (on the order of 7–8% missing per predictor). There was no evidence of extreme outliers that would require removal or transformation beyond standardization. The presence of some missing values and the lack of obvious one-variable predictors of cancer underlined the need for a robust modeling approach with proper data preprocessing, which we implemented as described below."
  },
  {
    "objectID": "Projects/SkinCancer/Example.knit.html#data-cleaning-and-missing-values",
    "href": "Projects/SkinCancer/Example.knit.html#data-cleaning-and-missing-values",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Data Cleaning and Missing Values",
    "text": "Data Cleaning and Missing Values\nAfter completing the exploratory analysis, we examined the dataset for missing values. We found that the overall data quality was relatively high, with most predictors containing approximately 7–8% missing values. No predictor exceeded 10% missingness, so eliminating variables or observations would have resulted in unnecessary data loss. To address missing values in a consistent manner, we applied the following imputation strategy: Numerical variables were imputed using the median, which is robust to outliers. Categorical variables were imputed using the most frequent category (mode). This approach allowed us to preserve all 50,000 observations in the training dataset while ensuring that the data were suitable for modeling. After imputation, no missing values remained in the predictors used for analysis.\n\n\n\nTable 1: Top ten variables with the highest proportion of missing values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing these association measures, all predictors were ranked from strongest to weakest. Variables with very weak association to the response variable were discarded, while stronger predictors were retained. This process reduced the dataset from 51 variables to 35 total variables, including the response variable. The final set consisted of 20 numerical predictors and 14 categorical predictors, while maintaining all original observations. This variable selection process helped simplify the modeling stage, reduced multicollinearity, and improved model interpretability, while retaining the most informative predictors for classification."
  },
  {
    "objectID": "Projects/SkinCancer/Example.knit.html#variable-selection",
    "href": "Projects/SkinCancer/Example.knit.html#variable-selection",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Variable Selection",
    "text": "Variable Selection\nGiven the large number of predictors, variable selection was necessary to reduce dimensionality and eliminate weak predictors that could introduce noise and increase the risk of overfitting. For categorical predictors, we used Cramér’s V to measure the strength of association between each categorical variable and the cancer outcome. Cramér’s V is based on the chi-squared statistic and produces values between 0 and 1, with larger values indicating stronger association. Here is a sample of the first 10 variables: For numerical predictors, we computed the point-biserial correlation, which measures the strength of association between a continuous predictor and a binary response variable. Numerical variables were ranked based on the absolute value of this correlation.\n\n\n\n\n\n\n\n\n[1] 35"
  },
  {
    "objectID": "Projects/SkinCancer/SkinCancer.knit.html#exploratory-data-analysis",
    "href": "Projects/SkinCancer/SkinCancer.knit.html#exploratory-data-analysis",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWe began by examining the overall structure and distributions in the training dataset. The response variable (Cancer) is binary (Benign or Malignant), and we observed a slight class imbalance: benign lesions were more frequent than malignant lesions in the training data. This imbalance means that a naive classifier that predicts all cases as benign would achieve a non-trivial accuracy, so careful model evaluation is necessary. We also inspected the distribution of key predictors. For example, the average age of patients with malignant lesions appeared higher than that of patients with benign lesions, consistent with the fact that skin cancer risk increases with age. We explored categorical risk factors as well: certain exposure-related factors (such as indicators of high UV exposure or tanning habits) were somewhat more prevalent in malignant cases, though there was significant overlap between the benign and malignant groups for most individual predictors. Overall, no single predictor showed a dramatic separation between malignant and benign lesions in isolation, suggesting that multiple factors in combination would be needed for effective prediction. In addition, we checked for missing data and outliers during the exploratory phase. We found that the dataset’s overall quality was high, with only a moderate amount of missing values (on the order of 7–8% missing per predictor). There was no evidence of extreme outliers that would require removal or transformation beyond standardization. The presence of some missing values and the lack of obvious one-variable predictors of cancer underlined the need for a robust modeling approach with proper data preprocessing, which we implemented as described below."
  },
  {
    "objectID": "Projects/SkinCancer/SkinCancer.knit.html#data-cleaning-and-missing-values",
    "href": "Projects/SkinCancer/SkinCancer.knit.html#data-cleaning-and-missing-values",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Data Cleaning and Missing Values",
    "text": "Data Cleaning and Missing Values\nAfter completing the exploratory analysis, we examined the dataset for missing values. We found that the overall data quality was relatively high, with most predictors containing approximately 7–8% missing values. No predictor exceeded 10% missingness, so eliminating variables or observations would have resulted in unnecessary data loss. To address missing values in a consistent manner, we applied the following imputation strategy: Numerical variables were imputed using the median, which is robust to outliers. Categorical variables were imputed using the most frequent category (mode). This approach allowed us to preserve all 50,000 observations in the training dataset while ensuring that the data were suitable for modeling. After imputation, no missing values remained in the predictors used for analysis.\n\n\n\nTable 1: Top ten variables with the highest proportion of missing values.\n\n\n\n\n\n\n\n\nVariable\nMissing (%)\n\n\n\n\nvitamin_d_supplement\n8.3\n\n\nphone_brand\n8.2\n\n\nskin_tone\n8.2\n\n\nsunscreen_spf\n8.2\n\n\npreferred_shoe_type\n8.2\n\n\nsunscreen_freq\n8.2\n\n\nresidence_lat\n8.2\n\n\ncommute_minutes\n8.1\n\n\nnear_high_power_cables\n8.1\n\n\nincome\n8.1\n\n\n\n (a) Training set variables.\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMissing (%)\n\n\n\n\nresidence_lon\n8.5\n\n\nphone_brand\n8.4\n\n\npreferred_shoe_type\n8.3\n\n\nuses_smartwatch\n8.3\n\n\nskin_tone\n8.3\n\n\nsmoking_status\n8.2\n\n\nincome\n8.2\n\n\nnear_high_power_cables\n8.2\n\n\nlesion_color\n8.2\n\n\nskin_photosensitivity\n8.1\n\n\n\n (b) Test set variables.\n\n\n\n\n\n\n\n\n\n\n\nUsing these association measures, all predictors were ranked from strongest to weakest. Variables with very weak association to the response variable were discarded, while stronger predictors were retained. This process reduced the dataset from 51 variables to 35 total variables, including the response variable. The final set consisted of 20 numerical predictors and 14 categorical predictors, while maintaining all original observations. This variable selection process helped simplify the modeling stage, reduced multicollinearity, and improved model interpretability, while retaining the most informative predictors for classification."
  },
  {
    "objectID": "Projects/SkinCancer/SkinCancer.knit.html#variable-selection",
    "href": "Projects/SkinCancer/SkinCancer.knit.html#variable-selection",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Variable Selection",
    "text": "Variable Selection\nGiven the large number of predictors, variable selection was necessary to reduce dimensionality and eliminate weak predictors that could introduce noise and increase the risk of overfitting. For categorical predictors, we used Cramér’s V to measure the strength of association between each categorical variable and the cancer outcome. Cramér’s V is based on the chi-squared statistic and produces values between 0 and 1, with larger values indicating stronger association. Here is a sample of the first 10 variables: For numerical predictors, we computed the point-biserial correlation, which measures the strength of association between a continuous predictor and a binary response variable. Numerical variables were ranked based on the absolute value of this correlation.\n\n\n\nTop 10 Categorical Predictors Ranked by Cramér’s V (Training Data)\n\n\nVariable\nCramér’s V\n\n\n\n\nfamily_history\n0.104\n\n\nskin_tone\n0.087\n\n\nsunscreen_freq\n0.066\n\n\nimmunosuppressed\n0.060\n\n\noccupation\n0.045\n\n\ntanning_bed_use\n0.041\n\n\nclothing_protection\n0.036\n\n\nskin_photosensitivity\n0.033\n\n\nhat_use\n0.028\n\n\nlesion_location\n0.011\n\n\n\n\n\n\nTop 10 Numerical Predictors Ranked by Point-Biserial Correlation (Training Data)\n\n\nVariable\nPoint-Biserial Correlation\n\n\n\n\nage\n0.1355\n\n\navg_daily_uv\n0.0599\n\n\nnumber_of_lesions\n0.0513\n\n\nsunburns_last_year\n0.0457\n\n\noutdoor_job\n0.0451\n\n\nlesion_size_mm\n0.0223\n\n\nsunscreen_spf\n-0.0174\n\n\nyears_lived_at_address\n0.0103\n\n\nincome\n0.0088\n\n\ndesk_height_cm\n0.0083\n\n\n\n\n\n[1] 35\n\n\n\n\n\nFinal Selected Predictors After Association-Based Variable Screening\n\n\nNo\nType\nVariable\nSelection Criterion\n\n\n\n\n1\nCategorical\nfamily_history\nCramér’s V &gt; 0.05\n\n\n2\nCategorical\nskin_tone\nCramér’s V &gt; 0.05\n\n\n3\nCategorical\nsunscreen_freq\nCramér’s V &gt; 0.05\n\n\n4\nCategorical\nimmunosuppressed\nCramér’s V &gt; 0.05\n\n\n5\nCategorical\noccupation\nCramér’s V &gt; 0.05\n\n\n6\nCategorical\ntanning_bed_use\nCramér’s V &gt; 0.05\n\n\n7\nCategorical\nclothing_protection\nCramér’s V &gt; 0.05\n\n\n8\nCategorical\nskin_photosensitivity\nCramér’s V &gt; 0.05\n\n\n9\nCategorical\nhat_use\nCramér’s V &gt; 0.05\n\n\n10\nCategorical\nlesion_location\nCramér’s V &gt; 0.05\n\n\n11\nCategorical\nfavorite_cuisine\nCramér’s V &gt; 0.05\n\n\n12\nCategorical\nmusic_genre\nCramér’s V &gt; 0.05\n\n\n13\nCategorical\nlesion_color\nCramér’s V &gt; 0.05\n\n\n14\nCategorical\nsunscreen_brand\nCramér’s V &gt; 0.05\n\n\n1\nNumerical\nage\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n2\nNumerical\navg_daily_uv\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n3\nNumerical\nnumber_of_lesions\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n4\nNumerical\nsunburns_last_year\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n5\nNumerical\noutdoor_job\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n6\nNumerical\nlesion_size_mm\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n7\nNumerical\nsunscreen_spf\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n8\nNumerical\nyears_lived_at_address\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n9\nNumerical\nincome\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n10\nNumerical\ndesk_height_cm\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n11\nNumerical\nresidence_lon\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n12\nNumerical\nexercise_freq_per_week\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n13\nNumerical\nzip_code_last_digit\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n14\nNumerical\ndistance_from_beach_km\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n15\nNumerical\nalcohol_drinks_per_week\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n16\nNumerical\nBMI\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n17\nNumerical\ncommute_minutes\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n18\nNumerical\nfrequency_doctor_visits_per_year\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n19\nNumerical\nresidence_lat\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n20\nNumerical\nmonthly_screen_time_minutes\nAbs. Point-Biserial Corr. &gt; 0.02"
  },
  {
    "objectID": "Projects/SkinCancer/Example.html",
    "href": "Projects/SkinCancer/Example.html",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "",
    "text": "This project develops and evaluates statistical learning models to classify skin lesions as benign or malignant using structured (non-image) predictors. Using a skin cancer dataset with 50,000 training observations and 20,000 test observations, we performed exploratory analysis, handled missing values via median (numeric) and mode (categorical) imputation, standardized numerical features, and reduced dimensionality through association-based feature screening. We then trained and compared a baseline logistic regression model with regularized variants (LASSO and elastic net logistic regression), tuning hyperparameters via cross-validation to balance generalization and performance under slight class imbalance.\nThe final model was an elastic net logistic regression with mixing parameter \\(\\alpha = 0.7\\) and 34 predictors, selected based on leaderboard performance. On the held-out test set, the model achieved 60.508% accuracy, outperforming naive baselines but remaining modest for a diagnostic setting. Overall, results suggest that structured demographic, behavioral, environmental, and clinical variables provide limited discriminative signal without lesion imaging features, highlighting both the potential utility of tabular predictors and the limitations of relying on them alone for skin cancer detection."
  },
  {
    "objectID": "Projects/SkinCancer/Example.html#exploratory-data-analysis",
    "href": "Projects/SkinCancer/Example.html#exploratory-data-analysis",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWe began by examining the overall structure and distributions in the training dataset. The response variable (Cancer) is binary (Benign or Malignant), and we observed a slight class imbalance: benign lesions were more frequent than malignant lesions in the training data. This imbalance means that a naive classifier that predicts all cases as benign would achieve a non-trivial accuracy, so careful model evaluation is necessary. We also inspected the distribution of key predictors. For example, the average age of patients with malignant lesions appeared higher than that of patients with benign lesions, consistent with the fact that skin cancer risk increases with age. We explored categorical risk factors as well: certain exposure-related factors (such as indicators of high UV exposure or tanning habits) were somewhat more prevalent in malignant cases, though there was significant overlap between the benign and malignant groups for most individual predictors. Overall, no single predictor showed a dramatic separation between malignant and benign lesions in isolation, suggesting that multiple factors in combination would be needed for effective prediction. In addition, we checked for missing data and outliers during the exploratory phase. We found that the dataset’s overall quality was high, with only a moderate amount of missing values (on the order of 7–8% missing per predictor). There was no evidence of extreme outliers that would require removal or transformation beyond standardization. The presence of some missing values and the lack of obvious one-variable predictors of cancer underlined the need for a robust modeling approach with proper data preprocessing, which we implemented as described below."
  },
  {
    "objectID": "Projects/SkinCancer/Example.html#data-cleaning-and-missing-values",
    "href": "Projects/SkinCancer/Example.html#data-cleaning-and-missing-values",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Data Cleaning and Missing Values",
    "text": "Data Cleaning and Missing Values\nAfter completing the exploratory analysis, we examined the dataset for missing values. We found that the overall data quality was relatively high, with most predictors containing approximately 7–8% missing values. No predictor exceeded 10% missingness, so eliminating variables or observations would have resulted in unnecessary data loss. To address missing values in a consistent manner, we applied the following imputation strategy: Numerical variables were imputed using the median, which is robust to outliers. Categorical variables were imputed using the most frequent category (mode). This approach allowed us to preserve all 50,000 observations in the training dataset while ensuring that the data were suitable for modeling. After imputation, no missing values remained in the predictors used for analysis.\n\n\n\nTable 1: Top ten variables with the highest proportion of missing values.\n\n\n\n\n\n\n\n\nVariable\nMissing (%)\n\n\n\n\nvitamin_d_supplement\n8.3\n\n\nphone_brand\n8.2\n\n\nskin_tone\n8.2\n\n\nsunscreen_spf\n8.2\n\n\npreferred_shoe_type\n8.2\n\n\nsunscreen_freq\n8.2\n\n\nresidence_lat\n8.2\n\n\ncommute_minutes\n8.1\n\n\nnear_high_power_cables\n8.1\n\n\nincome\n8.1\n\n\n\n (a) Training set variables.\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMissing (%)\n\n\n\n\nresidence_lon\n8.5\n\n\nphone_brand\n8.4\n\n\npreferred_shoe_type\n8.3\n\n\nuses_smartwatch\n8.3\n\n\nskin_tone\n8.3\n\n\nsmoking_status\n8.2\n\n\nincome\n8.2\n\n\nnear_high_power_cables\n8.2\n\n\nlesion_color\n8.2\n\n\nskin_photosensitivity\n8.1\n\n\n\n (b) Test set variables.\n\n\n\n\n\n\n\n\n\n\n\nUsing these association measures, all predictors were ranked from strongest to weakest. Variables with very weak association to the response variable were discarded, while stronger predictors were retained. This process reduced the dataset from 51 variables to 35 total variables, including the response variable. The final set consisted of 20 numerical predictors and 14 categorical predictors, while maintaining all original observations. This variable selection process helped simplify the modeling stage, reduced multicollinearity, and improved model interpretability, while retaining the most informative predictors for classification."
  },
  {
    "objectID": "Projects/SkinCancer/Example.html#variable-selection",
    "href": "Projects/SkinCancer/Example.html#variable-selection",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "Variable Selection",
    "text": "Variable Selection\nGiven the large number of predictors, variable selection was necessary to reduce dimensionality and eliminate weak predictors that could introduce noise and increase the risk of overfitting. For categorical predictors, we used Cramér’s V to measure the strength of association between each categorical variable and the cancer outcome. Cramér’s V is based on the chi-squared statistic and produces values between 0 and 1, with larger values indicating stronger association. Here is a sample of the first 10 variables: For numerical predictors, we computed the point-biserial correlation, which measures the strength of association between a continuous predictor and a binary response variable. Numerical variables were ranked based on the absolute value of this correlation.\n\n\n\nTop 10 Categorical Predictors Ranked by Cramér’s V (Training Data)\n\n\nVariable\nCramér’s V\n\n\n\n\nfamily_history\n0.104\n\n\nskin_tone\n0.087\n\n\nsunscreen_freq\n0.066\n\n\nimmunosuppressed\n0.060\n\n\noccupation\n0.045\n\n\ntanning_bed_use\n0.041\n\n\nclothing_protection\n0.036\n\n\nskin_photosensitivity\n0.033\n\n\nhat_use\n0.028\n\n\nlesion_location\n0.011\n\n\n\n\n\n\nTop 10 Numerical Predictors Ranked by Point-Biserial Correlation (Training Data)\n\n\nVariable\nPoint-Biserial Correlation\n\n\n\n\nage\n0.1355\n\n\navg_daily_uv\n0.0599\n\n\nnumber_of_lesions\n0.0513\n\n\nsunburns_last_year\n0.0457\n\n\noutdoor_job\n0.0451\n\n\nlesion_size_mm\n0.0223\n\n\nsunscreen_spf\n-0.0174\n\n\nyears_lived_at_address\n0.0103\n\n\nincome\n0.0088\n\n\ndesk_height_cm\n0.0083\n\n\n\n\n\n[1] 35\n\n\n\n\n\n\nFinal Selected Predictors After Association-Based Variable Screening\n\n\nNo\nType\nVariable\nSelection Criterion\n\n\n\n\n1\nCategorical\nfamily_history\nCramér’s V &gt; 0.05\n\n\n2\nCategorical\nskin_tone\nCramér’s V &gt; 0.05\n\n\n3\nCategorical\nsunscreen_freq\nCramér’s V &gt; 0.05\n\n\n4\nCategorical\nimmunosuppressed\nCramér’s V &gt; 0.05\n\n\n5\nCategorical\noccupation\nCramér’s V &gt; 0.05\n\n\n6\nCategorical\ntanning_bed_use\nCramér’s V &gt; 0.05\n\n\n7\nCategorical\nclothing_protection\nCramér’s V &gt; 0.05\n\n\n8\nCategorical\nskin_photosensitivity\nCramér’s V &gt; 0.05\n\n\n9\nCategorical\nhat_use\nCramér’s V &gt; 0.05\n\n\n10\nCategorical\nlesion_location\nCramér’s V &gt; 0.05\n\n\n11\nCategorical\nfavorite_cuisine\nCramér’s V &gt; 0.05\n\n\n12\nCategorical\nmusic_genre\nCramér’s V &gt; 0.05\n\n\n13\nCategorical\nlesion_color\nCramér’s V &gt; 0.05\n\n\n14\nCategorical\nsunscreen_brand\nCramér’s V &gt; 0.05\n\n\n1\nNumerical\nage\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n2\nNumerical\navg_daily_uv\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n3\nNumerical\nnumber_of_lesions\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n4\nNumerical\nsunburns_last_year\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n5\nNumerical\noutdoor_job\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n6\nNumerical\nlesion_size_mm\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n7\nNumerical\nsunscreen_spf\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n8\nNumerical\nyears_lived_at_address\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n9\nNumerical\nincome\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n10\nNumerical\ndesk_height_cm\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n11\nNumerical\nresidence_lon\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n12\nNumerical\nexercise_freq_per_week\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n13\nNumerical\nzip_code_last_digit\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n14\nNumerical\ndistance_from_beach_km\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n15\nNumerical\nalcohol_drinks_per_week\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n16\nNumerical\nBMI\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n17\nNumerical\ncommute_minutes\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n18\nNumerical\nfrequency_doctor_visits_per_year\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n19\nNumerical\nresidence_lat\nAbs. Point-Biserial Corr. &gt; 0.02\n\n\n20\nNumerical\nmonthly_screen_time_minutes\nAbs. Point-Biserial Corr. &gt; 0.02"
  },
  {
    "objectID": "Projects/SkinCancer/SkinCancer.knit.html",
    "href": "Projects/SkinCancer/SkinCancer.knit.html",
    "title": "Predicting Skin Cancer Using Statistical Learning Models",
    "section": "",
    "text": "⬇ Download PDF"
  }
]